{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"08HypothesisTesting.ipynb","provenance":[{"file_id":"1WT0ivGKPf-I8M4dRb8zn4tqqeb6zq4h6","timestamp":1620993412351},{"file_id":"1S5-ielUfmx-D-gM466zWtY-vnHrbdmAC","timestamp":1620400865271}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"f3CSmvHobUDK"},"source":["# HYPOTHESIS TESTING"]},{"cell_type":"markdown","metadata":{"id":"i1LOA6Ow3s9h"},"source":["> by Dr Juan H Klopper\n","\n","- Research Fellow\n","- School for Data Science and Computational Thinking\n","- Stellenbosch University"]},{"cell_type":"markdown","metadata":{"id":"Kxt2QWtQhkVU"},"source":["## PACKAGES USED IN THIS NOTEBOOK"]},{"cell_type":"code","metadata":{"id":"VpyUiInYhmUK"},"source":["import numpy as np\n","import pandas as pd\n","from scipy import stats"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5WxPEiAAhpoD"},"source":["import plotly.graph_objects as go\n","import plotly.express as px\n","import plotly.io as pio\n","pio.templates.default = 'plotly_white'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uyTKyl9btwW_"},"source":["## INTRODUCTION"]},{"cell_type":"markdown","metadata":{"id":"J4vUG9uhty6k"},"source":["In this notebook, we develop the intuition about the __scientific method__, which comprises the process of __hypotheses testing__, building on our knowledge gained in the previous notebook on randomess and sampling."]},{"cell_type":"markdown","metadata":{"id":"OMXBsqojt8XK"},"source":["We have seen previously then, that we can repeatedly sample from a population and build a distribution of a specific statistic based on each sample. Now we consider the place of a specific sampling in relation to the sampling distribution."]},{"cell_type":"markdown","metadata":{"id":"XGs67V6_udDV"},"source":["In reality, we only do a study once. We base our results on a sample and want to know how this relates to the population. Having calculated results pertaining to our sample of subjects, we and others who have read our results can infer the results to the population. To do this, we have to develop the understanding of how our one study results fits in with the distribution built if we could repeat the study many, many times over."]},{"cell_type":"markdown","metadata":{"id":"544ennq-u2mG"},"source":["Below, we work through some practical examples to build this understanding."]},{"cell_type":"markdown","metadata":{"id":"9sM-hgClu5-U"},"source":["## SAMPLE BASED ON PROPORTIONS"]},{"cell_type":"markdown","metadata":{"id":"C2_URQiGvAIW"},"source":["Consider a population with two mutually exclusive traits, these being A and B. It is known that trait A is present in $27$% of the population and the remainder, $73$%, have trait B. Our population size is $3000$. We take a random sample of $100$ subjects from the population and find that $13$% have trait A. We ask the question: _Is this proportion representative of the known population proportions?_"]},{"cell_type":"markdown","metadata":{"id":"ONwD90JQPG7o"},"source":["We start by creating the population using the numpy `choice` function. This time we add weights to each sample space element. The weights refer to the $27$% and the $73$%, expressed as fractions (that sum to $1.0$) and passed as a list to the `p` argument."]},{"cell_type":"code","metadata":{"id":"R2ivoZfpPO9h"},"source":["np.random.seed(42)\n","population = np.random.choice(['A', 'B'], size=3000, p=[0.27, 0.73])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"thOKQaC4PhuT"},"source":["This array of values can be stored in a dataframe object."]},{"cell_type":"code","metadata":{"id":"5KeVuTAfPmKt"},"source":["df = pd.DataFrame({'Trait':population})\n","df[:5] # Using indexing instead of df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G5AwXsN7Q1Hr"},"source":["The `unique` method shows the sample space elements and the `value_counts` return the frequency of each."]},{"cell_type":"code","metadata":{"id":"XSCyv-RIQQ8J"},"source":["df.Trait.unique()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i76-S6sw3_ch"},"source":["The `value_counts` method is used to return the frequency and realtive frequency (proportions) of the two sample space elements."]},{"cell_type":"code","metadata":{"id":"8IdHufoXQe2E"},"source":["df.Trait.value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OGgDo_aKaUjk"},"source":["df.Trait.value_counts(normalize=True) # Proportions"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"80UAp1NQP5lS"},"source":["Remember that we use a bar chart to visualize the frequency of nominal categorical variables and below we view the proportions of the two sample space elements in this example."]},{"cell_type":"code","metadata":{"id":"c85PiMf_QDJW"},"source":["px.bar(\n","    x=['A', 'B'],\n","    y=[0.27, 0.73],\n","    title='Relative frequency of traits in population',\n","    labels={\n","        'x':'Trait',\n","        'y':'Relative frequency'\n","    }\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RBqF2WXN4yCy"},"source":["Hovering over the two bars shows the $0.27$ and $0.73$ proportions as expected."]},{"cell_type":"markdown","metadata":{"id":"DT7nM22BWXF-"},"source":["Our imagined sample showed a relative frequency for the two traits as $0.13$ and $0.87$. A bar chart can visualize the research question proportions (_is the $0.13:0.87$ proportion representative_) and the population proportions ($0.13:0.73$)."]},{"cell_type":"code","metadata":{"id":"FFNa5GylWbz_"},"source":["go.Figure(\n","    data=go.Bar(\n","        x=['A', 'B'],\n","    y=[0.27, 0.73],\n","    name='Population proportions'\n","    )\n",").add_trace(\n","    go.Bar(\n","        x=['A', 'B'],\n","        y=[0.13, 0.87],\n","        name='Research proportions'\n","    )\n",").update_layout(title='Population and research proportions of traits',\n","                xaxis={'title':'Traits'},\n","                yaxis={'title':'Relative frequency'},\n","                bargap=0.2, # gap between bars of adjacent location coordinates\n","                bargroupgap=0.1) # gap between bars of the same location coordinates"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x-KSctBnOwYm"},"source":["As before, we can sample from the population repeatedly and visualize a distribution of a specific statistic. In this case, our statistic can be the percentage (or fraction) of the sample with trait A."]},{"cell_type":"markdown","metadata":{"id":"LP2HhApfS1me"},"source":["The `choice` function can select the specified numer of random values from an array."]},{"cell_type":"code","metadata":{"id":"H2MmM8zXS_6K"},"source":["np.random.choice(population, size=100) # Selecting 100 random subjects"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SxBuCkHzTFNz"},"source":["The numpy `unique` function return the sample space elements and with the `return_counts` argument set to `True`, it returns a $2$-tuple. The first element is an array of the sample space elements and the second is an array of the frequencies of each of the sample space elements."]},{"cell_type":"code","metadata":{"id":"YV35fA3HTcoD"},"source":["np.unique(np.random.choice(population, size=100), return_counts=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4p5dRXqmTf8F"},"source":["We need the first element from the second array. We do this using indexing."]},{"cell_type":"code","metadata":{"id":"I0ygbQICR1ww"},"source":["np.unique(np.random.choice(population, size=100), return_counts=True)[1][0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T4fEYnO5TmcJ"},"source":["Since our statistic is the proprotion of this first element above, we can divide it by the sample size."]},{"cell_type":"markdown","metadata":{"id":"6Yt5T1otPADa"},"source":["Below, we sample from the population $5000$ times and record the proportion of subjects with trait A."]},{"cell_type":"code","metadata":{"id":"4iiSzSMrRJGJ"},"source":["count = [] # Empty list to hold all the trait A proportions\n","n = 100 # Samples size\n","\n","for i in range(5000):\n","  count.append(np.unique(np.random.choice(population, size=n), return_counts=True)[1][0] / n)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mVPnN970VLsl"},"source":["No we look at a histogram of all the trait A proportions. We also add a red vertical line at our original $13$%."]},{"cell_type":"code","metadata":{"id":"lC05pWwsT6N6"},"source":["go.Figure(\n","    data=go.Histogram(\n","        x=count,\n","        nbinsx=20,\n","        name='Proprotions'\n","    )\n",").add_trace(go.Scatter(\n","    x=[0.13, 0.13],\n","    y=[0, 800],\n","    mode='lines',\n","    name='Original proportion'\n",")).update_layout(title='Distribution of proportions of trait A',\n","                 xaxis={'title':'Proportion of trait A'},\n","                 yaxis={'title':'Frequency'})"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4eV-ygoY6J2h"},"source":["Our imagined proportion of $0.13$ occured with a very low frequency according to the histogram. It was unlikely to have such a proportion. We can actual give a proportion of times that we had proportions of $0.13$ and smaller in our simulation."]},{"cell_type":"code","metadata":{"id":"mUx-G25HdxWW"},"source":["np.sum(np.array(count) < 0.13) / 5000"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GWAkj1daXwob"},"source":["A statistical test to see if a proportion in a sample is different from known proportions is the $\\chi^{2}$ test for proportions. For this we use the `chisquare` function in the stats module in scipy. We pass two arguments, `f_obs` and `f_exp`. The values in our example will be two list, each with two elements. We multiply the proportions by the sample size in both cases."]},{"cell_type":"code","metadata":{"id":"D81XcvwkY14U"},"source":["stats.chisquare(\n","    f_obs=[0.13 * 100, 0.87 * 100],\n","    f_exp=[0.27 * 100, 0.73 * 100]\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hijDdSJmaxHj"},"source":["The $13$% from the research question was an unlikely finding based on the histogram. Expressed as a _p_ value using the proportion test, we see a very small value, which is a reflection of the histogram and the proportion of $0.0004$ that we caluclated."]},{"cell_type":"markdown","metadata":{"id":"OlmVGQvnbgmR"},"source":["## EXAMPLE BASED ON A DIFFERENCE IN MEANS"]},{"cell_type":"markdown","metadata":{"id":"ot_BfPsyblVH"},"source":["In this example we know the value of a continous numerical variable in each subject in a population. The sample space elements are on the interval $\\left[0,100\\right)$. The distribution of the elements takes on a uniform distribution in the popluation."]},{"cell_type":"code","metadata":{"id":"ERoBHJrTc3Wy"},"source":["# The random function returns a value between 0 and 1\n","population = np.random.random(3000) * 100"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8tbaNDJEdDdq"},"source":["Imagine then that the population is spread over two neighbouring towns. A researcher suspects that there is a difference in the value of this variable between the two towns (not having access to all the known values as we do). A random sample of $100$ individuals from each town results in a mean value of $45.3$ for town A and $52.8$ for town B. How can the researcher asses this differrence?"]},{"cell_type":"markdown","metadata":{"id":"A-BaoCTTdta6"},"source":["Once again, we resample repeatedly from the two towns and represent this simulation below. The test statistic is _difference in means_, with the researcher's difference being $52.8-45.3=7.5$. Since there is no natural order between these two towns, we might also have a difference of $45.3-52.8=-7.5$."]},{"cell_type":"markdown","metadata":{"id":"PjtHYwHphCx9"},"source":["We code our repeated sampling and visualize the distribution of our test statistic which is _difference in means_. We also visualize the researcher's difference in means."]},{"cell_type":"code","metadata":{"id":"aJAulSQPeJPs"},"source":["difference = [] # Empty list to be populated by differences in for loop\n","\n","for i in range(1000): # Loop 1000 times\n","  sample_A_ave = np.mean(np.random.choice(population, size=100)) # Mean of 100 samples from town A\n","  sample_B_ave = np.mean(np.random.choice(population, size=100)) # Mean of 100 samples from town B\n","  difference.append(sample_A_ave - sample_B_ave) # Append difference to list on each loop"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nnhytjv7ev0F"},"source":["go.Figure(\n","    data=go.Histogram(\n","        x=difference,\n","        name='Difference distribution'\n","    )\n",").add_trace(\n","    go.Scatter(\n","        x=[-7.5, -7.5],\n","        y=[0, 100],\n","        name='A-B'\n","    )\n",").add_trace(\n","    go.Scatter(\n","        x=[7.5, 7.5],\n","        y=[0, 100],\n","        name='B-A'\n","    )\n",").update_layout(\n","    title='Distribution of the difference in means',\n","    xaxis={'title':'Difference in means'},\n","    yaxis={'title':'Frequency'}\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wst8b1B9evFF"},"source":["It is worthwhile to note that the distribution of means seem to take on a bell-shaped curve, despite the fact that the variable was distributed uniformly in the population. We also note that the difference found by the researcher seems to have been _uncommon_. As with the proportion test above, there are statistical tests that can enumerate just how _uncommon_ this finding was."]},{"cell_type":"markdown","metadata":{"id":"V1lr51JqiIQ8"},"source":["Assessing the research's finding follows the processes of the scientific method. These processes are termed __hypothesis testing__ and is the corner stone of the scientific method."]},{"cell_type":"markdown","metadata":{"id":"oRmgTEQfd_6j"},"source":["## HYPOTHESES TESTING"]},{"cell_type":"markdown","metadata":{"id":"D9QYigrAinIq"},"source":["In hypothesis testing we have two views of our research question. We can see this as two views about how the data was generated. The two views are termed __hypotheses__. There are two hypotheses, the null and the alternative hypothesis."]},{"cell_type":"markdown","metadata":{"id":"wpXG3B-si3TO"},"source":["The __null hypothesis__ takes on a conservative approach. It states that the data was generated from clearly defined parameters and assumptions about randomness. Any deviation from the data generated from the null hypothesis is taken to be purely by chance. In our first eaxmple above, there was the assumption that the proportions of the traits in the population were $0.27:0.73$. In the second example it was that the data was on an interval from a uniform distribution. We simulated the data under these assumptions about randomness."]},{"cell_type":"markdown","metadata":{"id":"4dibWNuhkIf-"},"source":["The __alternative hypothesis__ states that something other than chance lead to a difference in the data from the prediction of the model under the null hypothesis."]},{"cell_type":"markdown","metadata":{"id":"vY694U7ckk5o"},"source":["Until we collect and analyze any data from a selected sample, we stand by the null hypothesis (about the distribution of the data in the population from which the sampe was taken). To _choose between_ the two hypotheses, we require a statistic, termed the __test statistic__. In our example above, it was the proportion of the first trait and in the second, the difference in means."]},{"cell_type":"markdown","metadata":{"id":"PVQzydKElKXI"},"source":["The null hypothesis in the first example could be stated as: _The proportion of the A trait in the sample is_ $0.27$. The alternative hypothesis would then be: _The proportion of trait A in the sample is not $0.27$_."]},{"cell_type":"markdown","metadata":{"id":"gvTKfeF_lgAf"},"source":["The null hypothesis in the second example could be stated as :_There is no difference in the means of the variable between the two towns_. The alternative hypothesis would then be: _There is a difference in the mean between the two towns_. Note that we do not subscribe which town has an average more or less than the other. This is referred to as a __two-tailed alternative hypothesis__. Depending on the order of subtraction, we would get a positive or a negative difference (unless they are equal, which is usually unlikely)."]},{"cell_type":"markdown","metadata":{"id":"xtlM-Aq4mMzl"},"source":["Simulation of possible test statistics using repeated sampling gave us a good idea of the distribution of the statistic and we could visualize _how likely_ the research statisic was (the single instance of sampling that the researcher performed)."]},{"cell_type":"markdown","metadata":{"id":"kMW1jGA2myqT"},"source":["The question remains: _How unlikely (away from the most often found test statistics found during repeated sampling) must the research test statistic be before we reject the null hypothesis and accept the alternative hypothesis_? Note that if the test statistic is among the most often found test statistics, we fail to reject the null hypothesis. We can not accept or prove the null hypothesis. It is simply the finding given the assumptions."]},{"cell_type":"markdown","metadata":{"id":"6yhE5Rr8nbY8"},"source":["By convention, we choose a _cut off_ value to make this decision. This value is termed an $\\alpha$ value and for various disciplines this is set at $0.05$, or $0,01$, or even much smaller (particle physics comes to mind)."]},{"cell_type":"markdown","metadata":{"id":"7Rc7EZownvoq"},"source":["The mathematics that underlies the statistics for these tests consider a probability density function (PDF) and a cumulative distribution function (CDF) (in the case of constinuous numerical functions). The total area under the curve of the PDF is $1.0$. In the case of the second example above, the area to the left of the red line added to the area to the right of the green line would represent the _p_ value (to some approximation relevant to this discussion as a histogram is not a PDF). If this is less than the chosen $\\alpha$ value, we reject the null hypothesis and accept the alternatibe hypothesis. Otherwise, we fail to reject the null hypothesis. Visually, the latter represents a _likely_ statistic and the former an _unlikely statistic_. This is how some disciplines express statistical significance or finding a ststistically significant result."]},{"cell_type":"markdown","metadata":{"id":"4zyb29y0ooWp"},"source":["Note that the $\\alpha$ value is ARBITRARY."]},{"cell_type":"markdown","metadata":{"id":"ClRCWplbeHKy"},"source":["As proper researchers, we have two hyothesis.  With respect to continuous numerical variables for instance, the **null hypothesis** is our default and we state that there is no difference between the means, unless we collected evidence and it proves otherwise.  Our **alternative hypothesis** is just that.  There is a difference in the means.  When the evidence (calculations) is not sufficient, we fail to reject the null hypothesis.  If the evidence is there, we reject the null hypothesis and accept the alternative hypothesis.  To do all this, we need an $\\alpha$ value. We review how does this all fit together."]},{"cell_type":"markdown","metadata":{"id":"bImYTIODdkR8"},"source":["What we have learned above and remember form the previous notebook, is that our difference is but one of many, that will fall somewhere on a sampling distribution.  Some test statistics occur commonly and some not so commonly.  With a specific parameters (peratining to the test we use), we can construct a probability density function (PDF) and plot it.  We find out where on the plot to draw our two horizontal lines that will show an area under the curve (using an $\\alpha$ value of $0.05$) to the left of the left-sided symmetrical line of 0.025 (2.5%), and another 2.5% to the right of the righ-sided symmetric vertical line.  We calculated these symmetrical values using the `ppf` function (something we will later call critical values).  The 2.5% reflects half of our $\\alpha = 0.05$ decision."]},{"cell_type":"markdown","metadata":{"id":"yaInnVh0faEU"},"source":["Finally, we convert our test statistic appropriately and reflect it along the other side of the curve through symmetry.  Our hypothesis is a two-tailed hypothesis (there is a difference), which depends which mean we subtract from which (resulting in a positive or a negative value)."]},{"cell_type":"markdown","metadata":{"id":"8VO3oeQIft2b"},"source":["Finally, we look towards negative and positive infinity from our *t* stistic lines and calculate the area under the curve or *p* value.  If the *t* statistics are outside of two 5% lines (each at 2.5%), we will have a small *p* value (area).  Given all the possible outcomes (differences in means), this would indicate that we discovered one of the lesser probable ones and reject our null hypothesis.  We state that the difference is significant and (if the new drug had more of a reduction), we declare it it different from the old drug.  If not, we fail to reject the null ypothesis and state that the two drugs are equally effective (using all these terms loosely)."]},{"cell_type":"markdown","metadata":{"id":"NE93D3Djgcc1"},"source":["To be sure, we also get one-tailed hypothesis.  That is where we can make a strong argument that one mean will be more than the other.  We then do not reflect the statistic one either side."]},{"cell_type":"markdown","metadata":{"id":"SHTPs67gyZgQ"},"source":["## STATING A HYPOTHESIS BASED ON A RESEARCH QUESTION"]},{"cell_type":"markdown","metadata":{"id":"FFKvEPzlygIH"},"source":["Now that we know about hypothesis testing, let's put it to the test.  More examples always help.  We imagine a study where we are investigating a new intervention.  We create two groups.  In one, the participants receive a placebo intervention and in the other, a new intervention.  In each group we meassure a certain variable for each individual.  Our research question is: *Is there a difference in the variable between the placebo and intervention groups?*"]},{"cell_type":"markdown","metadata":{"id":"o1ZropuvzDlw"},"source":["It is an absolute must that we are able to state our research questions in a way that we can use hypothesis testing.  In our research question above, we have a single variable and two groups.  One group will recieve the new intervention and the other, a placebo (an empty) intervention.  We will collect data point values for a variable following the interventions (real intervention and placebo) and measure the difference in the data between the two groups."]},{"cell_type":"markdown","metadata":{"id":"u4SpAovjzruv"},"source":["Our null hypothesis for this research question is: *There is no difference in the data for the variable between the two groups*.  The null hypothesis is sometimes written as $H_0$."]},{"cell_type":"markdown","metadata":{"id":"CqzgMRTe0cXB"},"source":["How will we do this comparison, though?  Well, that depends on the data type of the variable.  Let's assume that it is a continuous numerical variable.  If the assumptions for the use of parametric tests are met (which we will investigate in the next notebook), it means that we will compare the means of the variable between the two groups, i.e. the mean is our test statistic.  If the placebo group has a mean for the variable of $\\bar{X}_1$ and the new intervention group has a mean of $\\bar{X}_2$, then we would state our nul hypothesis as: ${H}_{0}: \\bar{X}_{1} = \\bar{X}_{2}$.  The means are equal."]},{"cell_type":"markdown","metadata":{"id":"_xCY2mGT07XX"},"source":["Our alternative hypothesis would then be that the means are not the same.  This is written as: ${H}_{\\alpha}: \\bar{X}_{1} \\ne \\bar{X}_{2}$.  What we have here is a two-tailed hypothesis.  We merely state that there is a difference and we are not concerned with which group will have a mean of more or less than the other."]},{"cell_type":"markdown","metadata":{"id":"iqE2lbou1M-Q"},"source":["The aim is now to collect data and see if there is enough evidence to reject the null hypothesis and therefor accept the alternative hypothesis or, in the case that there is not enough evidence, to fail to reject the null hypothesis.  These are important concepts.  We never prove the null hypothesis.  In fact, the sampling distributions on which we will base our statistical tests are created in view of the fact that no difference exists.  Our study merely finds an unlikely difference or it does not."]},{"cell_type":"markdown","metadata":{"id":"0-sqDk6L1x6P"},"source":["To make the distinction between enough evidence or not, we set an $\\alpha$ value.  This is usually $0.05$.  If the area under the curve (in actual fact, the cummulative distribution function value) is less than the $\\alpha$ value, i.e. a *p* value of less than the $\\alpha$ value, we reject the null hypothesis and accept the alternative hypothesis.  If not, then we fail to reject the null hypothesis."]},{"cell_type":"markdown","metadata":{"id":"ef4Yjd0H2Z9n"},"source":["## GENERATING DATA"]},{"cell_type":"markdown","metadata":{"id":"I7lKlw2G2dHv"},"source":["For the sake of some pratice, let's generate our own simulated data for our research question.  We create two computer variables, one for the placebo group and one for the intervention group.  Both sets of data point values for our imaginary variable will comes from a normal distribution."]},{"cell_type":"markdown","metadata":{"id":"tnpfKPI626Qx"},"source":["For the intervention group, we choose a mean of $50$ and a standard deviation of $5$ and for the placebo group, a mean of $48$ and a standard deviation of $7$.  We use the `norm.rvs()` function to generate the data."]},{"cell_type":"code","metadata":{"id":"AKJY5xKe1tUP"},"source":["intervention = stats.norm.rvs(loc=50,\n","                              scale=5,\n","                              size=100,\n","                              random_state=3)  # For reproducible results\n","\n","placebo = stats.norm.rvs(loc=48,\n","                         scale=7,\n","                         size=100,\n","                         random_state=3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bIvIVv1ZBTho"},"source":["Just as a sneak peek at how easy it is to calculate a *p* value, take a look the the line of code below.  It returns a *t* statistic and a *p* value.  Don't stare at it for too long, though.  We will take the long route so that we understand how this is calculated."]},{"cell_type":"code","metadata":{"id":"cDOc6kPrBeh0"},"source":["stats.ttest_ind(intervention, placebo)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uSwCj0Ty4CHo"},"source":["Let's summarise and visualise our data.  First, we look at the mean and then the standard deviation of the variable for each group."]},{"cell_type":"code","metadata":{"id":"gjd2OlT237vQ"},"source":["print('Mean for intervention group: ', '\\t', intervention.mean(), '\\n',\n","      'Mean for placebo group: ', '\\t', placebo.mean())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BJkp5eyI4lsu"},"source":["print('Standard deviation for intervention group: ', '\\t', intervention.std(), '\\n',\n","      'Standard deviation for placebo group: ', '\\t', placebo.std())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G_8u-Wvko7bd"},"source":["A box-and-whisker plot will be more intuitive."]},{"cell_type":"code","metadata":{"id":"z-gjBsh54tCG"},"source":["box_fig = go.Figure()\n","\n","box_fig.add_trace(go.Box(y=intervention,\n","                         name='Intervention group',\n","                         boxmean='sd',\n","                         boxpoints='suspectedoutliers'))\n","\n","box_fig.add_trace(go.Box(y=placebo,\n","                         name='Placebo group',\n","                         boxmean='sd',\n","                         boxpoints='suspectedoutliers'))\n","\n","box_fig.update_layout(title='Box-and-whisker plot',\n","                      xaxis=dict(title='Group'),\n","                      yaxis=dict(title='Variable value'))\n","\n","box_fig.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V8ygc1iqpBV8"},"source":["Take a guess.  Do you think there is a statistically significant difference between the means?"]},{"cell_type":"markdown","metadata":{"id":"OrSucWbE5zrW"},"source":["## IS THERE A DIFFERENCE?"]},{"cell_type":"markdown","metadata":{"id":"Or304VIV53pe"},"source":["The question now is whether there is a difference between the calculated means of $49.5$ and $47.2$.  Well the difference in means are shown below.  We can subtract one mean from the other in either order."]},{"cell_type":"code","metadata":{"id":"_XIn0gMD5OmN"},"source":["intervention.mean() - placebo.mean()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nrMa_E_Y6NMO"},"source":["placebo.mean() - intervention.mean()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"91aXmVBV6acd"},"source":["We do remember from the previous notebook that this difference in means is but one of many possible means.  Since we don't know the standard deviation for our variable in the population (we did not simulate a whole population and sample from it), we will make use of the *t* distribution. It is a theoretical sampling distribution based only on the sample size (known as the degrees of freedom). We have $200$ participants in our study divided into two groups.  To set up the *t* distribution, we need to known the degrees of freedom.  This would simply be $200-2=198$.  The sample size minus the number of groups."]},{"cell_type":"markdown","metadata":{"id":"aGW9ifBa637m"},"source":["Below, we create *t* distribution for $198$ degrees of freedom."]},{"cell_type":"code","metadata":{"id":"GxP27y56642H"},"source":["t_vals = np.linspace(-3, 3, 200)  # Generating some values for the x-axis\n","t_pdf_vals = stats.t.pdf(t_vals, 198)  # Calculating the PDF value for each of the x-axis values\n","\n","\n","t_dist_fig = go.Figure()\n","\n","t_dist_fig.add_trace(go.Scatter(x=t_vals,\n","                                y=t_pdf_vals,\n","                                mode='lines',\n","                                name='t distribution'))\n","\n","t_dist_fig.update_layout(title='t distribution',\n","                         xaxis=dict(title='t values'),\n","                         yaxis=dict(title='PDF'))\n","\n","t_dist_fig.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o7jz3LP87-Fp"},"source":["Now we have to express our difference in means as a *t* statistic.  We can use equation (1) below, where $\\Delta \\bar{X}$ is the difference in means."]},{"cell_type":"markdown","metadata":{"id":"Zi_nG4We8b7G"},"source":["$$ t = \\frac{\\Delta \\bar{X}}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}} \\tag{1} $$"]},{"cell_type":"markdown","metadata":{"id":"AJaYcYnz8xof"},"source":["Let's go for a difference of $-2.217$, placebo group mean minus intervention group mean."]},{"cell_type":"code","metadata":{"id":"wzvfnJ4m6Sxm"},"source":["t_stat = (placebo.mean() - intervention.mean()) / (np.sqrt((placebo.std()**2 / 100) + (intervention.std()**2 / 100)))\n","t_stat"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"btq2fQzx_dOF"},"source":["Now we have the *t* statistic value for the placebo group mean  minus the intervention group mean.  We can plot this as a horizontal line (in red below)."]},{"cell_type":"code","metadata":{"id":"C9JxhZKc90Sf"},"source":["t_dist_fig.add_trace(go.Scatter(\n","    x=[t_stat, t_stat],\n","    y=[0,0.4],\n","    name='Placebo - Intervention',\n","    mode='lines'\n","))\n","\n","t_dist_fig.update_layout(title='Difference in means')\n","\n","t_dist_fig.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yFGU2FOj_92l"},"source":["We have to refelect this on the other side as well for a two-tailed hypothesis.  Our alternative hypothesis was that there was a difference, only."]},{"cell_type":"code","metadata":{"id":"oCaGDc_M_SmH"},"source":["t_dist_fig.add_trace(go.Scatter(\n","    x=[-t_stat, -t_stat],\n","    y=[0,0.4],\n","    name='Intervention - Placebo',\n","    mode='lines'\n","))\n","\n","t_dist_fig.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AOebBfbqAcvF"},"source":["If we look at the area under the curve from negative infinity to the red line and from the green line to positive infinity, we are looking at the *p* value.  To calculate this, we will simply calculate the value of the (red line) *t* statistic using the cummulative distribution function, `t.cdf`, and multiply it by $2$."]},{"cell_type":"code","metadata":{"id":"8BkaQSAKAJQV"},"source":["stats.t.cdf(t_stat, 198) * 2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p1oYbc0lA_iU"},"source":["A *p* value of $0.02$ (rounded).   Smaller than our chosen $\\alpha$ value of $0.05$, for sure.  This is because these *t* statistic values fall outsde of the critical *t* values.  These are the values that would represent 2.5% of the area under the curve on either side.  We add them below."]},{"cell_type":"code","metadata":{"id":"O45mM5oOqldS"},"source":["t_crit = stats.t.ppf(0.025, 198)\n","\n","t_dist_fig.add_trace(go.Scatter(\n","    x=[t_crit, t_crit],\n","    y=[0,0.4],\n","    name='Critical t statistic',\n","    mode='lines'\n","))\n","\n","t_dist_fig.add_trace(go.Scatter(\n","    x=[-t_crit, -t_crit],\n","    y=[0,0.4],\n","    name='Critical t statistic',\n","    mode='lines'\n","))\n","\n","t_dist_fig.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cjBXZUJEq5y0"},"source":["To the left of the purple line and to the right of the orange line, we find our areas of rejection.  Each of these areas are 2.5% of the area under the curve.  See how our *t* statistic(s) are within the areas of rejection."]},{"cell_type":"markdown","metadata":{"id":"XKOfcqHnqcBU"},"source":["Finally, we have enough evidence to reject our null hypothesis and accept our alternative hypothesis.  There is a statistically significant difference in our variable compared between the two groups."]},{"cell_type":"markdown","metadata":{"id":"x7eJndnZQ3CT"},"source":["## ONE-TAILED HYPOTHESIS"]},{"cell_type":"markdown","metadata":{"id":"PQOYTDvLQ8Rx"},"source":["It might very well be that our alternative hypothesis is one-tailed.  This can be a dangerous decision.  We have to be able to make a reasonable argument to convince our peers that we expected that one mean would be higher or lower than the other.  For a problem such as ours (above) that would mean that the *p* value is divided by $2$.  It can be dangerous and tempting to change our minds after the analysis and go for a one-tailed alternative hypothesis, especially if the *p* value was close to $0.05$.  A hypothesis must be set during the study design and we cannot change that after the fact."]},{"cell_type":"markdown","metadata":{"id":"hZld3xLKR-Ml"},"source":["Just for argument's sake let's look at the one-tailed hypotheses.  First, a reminder of the two means."]},{"cell_type":"code","metadata":{"id":"ssH15CVNL61i"},"source":["print('Mean for intervention group: ', '\\t', intervention.mean(), '\\n',\n","      'Mean for placebo group: ', '\\t', placebo.mean())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tmew0McCSlRp"},"source":["Let's make group 1 the placebo group and group 2 the intervention group.  For our first scenario, we state that the mean of the placebo group is greater than or equal to the mean of the intervention group.  The alternative hypothesis is then that the mean of the placebo group is less than that of the intervention group.  We state this in eqution (2) below, where $\\bar{X}_1$ is the mean of the placebo group and $\\bar{X}_2$ is the mean of the intervention group.  To be clear, the alternative hypothesis is the one we are *hoping* to show."]},{"cell_type":"markdown","metadata":{"id":"bzr7Q1hGVmqp"},"source":["$$ {H}_{0}: \\bar{X}_{1} \\ge \\bar{X}_{2} \\\\ {H}_{\\alpha}: \\bar{X}_{1} < \\bar{X}_{2}  \\tag{2} $$"]},{"cell_type":"markdown","metadata":{"id":"N7vlmb_JToNS"},"source":["We need a critical *t* value which represents an area under the probability density curve which represents $0.05$ of the total area (to the left).  We can calculate this using the `ppf()` function, which we use below for $198$ degrees of freedom."]},{"cell_type":"code","metadata":{"id":"XVLX3lYjSN7B"},"source":["t_crit = stats.t.ppf(0.05, 198)\n","t_crit"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xqGIf1HaT9pI"},"source":["We can now plot this together with our actual *t* statistic."]},{"cell_type":"code","metadata":{"id":"sxmZr1RSShPJ"},"source":["t_dist_fig = go.Figure()\n","\n","t_dist_fig.add_trace(go.Scatter(x=t_vals,\n","                                y=t_pdf_vals,\n","                                mode='lines',\n","                                name='t distribution'))\n","\n","t_dist_fig.add_trace(go.Scatter(\n","    x=[t_stat, t_stat],\n","    y=[0,0.4],\n","    name='Placebo - Intervention',\n","    mode='lines'))\n","\n","t_dist_fig.add_trace(go.Scatter(\n","    x=[t_crit, t_crit],\n","    y=[0,0.4],\n","    name='Critical t value',\n","    mode='lines'))\n","\n","t_dist_fig.update_layout(title='One-tailed alternative hypothesis',\n","                         xaxis=dict(title='t values'),\n","                         yaxis=dict(title='PDF'))\n","\n","t_dist_fig.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xmh4eQ8sUz-x"},"source":["The green line is the critical *t* value.  The area under the curve to the left is $0.05$ and not just $0.025$.  We need not split the area up into two symmetrical sides.  We see that our test statistic is much less that the critical *t* value.  The *p* value is calculate below."]},{"cell_type":"code","metadata":{"id":"NSsag3bqUc9-"},"source":["stats.t.cdf(t_stat, 198)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dnn7RBqPVUb4"},"source":["This is half of our original, two-tailed, *p* value.  Our null hypothesis was that the mean of the placebo group was equal to or larger than the intervention group, but we found it to be less.  We reject the null hypothesis and accept the alternative hypothesis."]},{"cell_type":"markdown","metadata":{"id":"IornQCDpsQrO"},"source":["We can also state the *opposite* one-tailed alternative hypothesis, (3)."]},{"cell_type":"markdown","metadata":{"id":"B9n9s1RlViaY"},"source":["$$ {H}_{0}: \\bar{X}_{1} \\le \\bar{X}_{2} \\\\ {H}_{\\alpha}: \\bar{X}_{1} > \\bar{X}_{2}  \\tag{3} $$"]},{"cell_type":"markdown","metadata":{"id":"c8JPm4x3Vs8Y"},"source":["The critical critical *t* value is now calculated below, where we look at $0.05$ of the area under the curve on the positive side."]},{"cell_type":"code","metadata":{"id":"G0MLg7MLVSaA"},"source":["t_crit = stats.t.ppf(0.95, 198)\n","t_crit"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9PtUDRM6V4fo"},"source":["t_dist_fig = go.Figure()\n","\n","t_dist_fig.add_trace(go.Scatter(x=t_vals,\n","                                y=t_pdf_vals,\n","                                mode='lines',\n","                                name='t distribution'))\n","\n","t_dist_fig.add_trace(go.Scatter(\n","    x=[t_stat, t_stat],\n","    y=[0,0.4],\n","    name='Placebo - Intervention',\n","    mode='lines'))\n","\n","t_dist_fig.add_trace(go.Scatter(\n","    x=[t_crit, t_crit],\n","    y=[0,0.4],\n","    name='Critical t value',\n","    mode='lines'))\n","\n","t_dist_fig.update_layout(title='One-tailed alternative hypothesis',\n","                         xaxis=dict(title='t values'),\n","                         yaxis=dict(title='PDF'))\n","\n","t_dist_fig.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U1591sgvXE2i"},"source":["Our rejection region is now to the right of the green line, but our difference is still the red line, very much outside the rejection area and we fail to reject the null hypothesis.  Our *p* value is caluclated below, where we subtract the value (to the left of the red line) from the total area under the curve."]},{"cell_type":"code","metadata":{"id":"HmdzRkG8XAkY"},"source":["1 - stats.t.cdf(t_stat, 198)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L7uWb90oBohS"},"source":["## CONCLUSION"]},{"cell_type":"markdown","metadata":{"id":"XdZH61mzBqIq"},"source":["In this notebook we were introduced to hypothesis testing and some specific statistical tests to build an intuition of how hypothesis testing works. In understanding Data Science, we want to learn more about uncertainty, though. In the next notebook we review what we have learnt here, but start to introduce more concepts."]},{"cell_type":"code","metadata":{"id":"DkqurWIfBpoy"},"source":[""],"execution_count":null,"outputs":[]}]}