{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"14MachineLearningDecisionTreeRandomForest.ipynb","provenance":[{"file_id":"19i7GXePQaHbtateha3Ij87tqbq_GPFmc","timestamp":1622193685811}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Ga__pLIcQc48"},"source":["# MACHINE LEARNING WITH DECISION TREES AND RANDOM FORESTS"]},{"cell_type":"markdown","metadata":{"id":"fJ_SOPwuQgvH"},"source":[">by Dr Juan H Klopper\n","\n","- Research Fellow\n","- School for Data Science and Computational Thinking\n","- Stellenbosch University"]},{"cell_type":"markdown","metadata":{"id":"fbwGcoSdQpN9"},"source":["## INTRODUCTION"]},{"cell_type":"markdown","metadata":{"id":"LwTacqL4QrhM"},"source":["__Random Forests__ and __gradient boosted trees__ are commonly used machine learning (ML) techniques used in classification and regression problems. They have the advantage over some other ML techniques in that the models are interpretable."]},{"cell_type":"markdown","metadata":{"id":"6Gh9MZGJaGwe"},"source":["The basic building block of a random forest is a __decision tree__. The term decision tree is almost self-explanatory. The algorithm builds a tree structure by making repeated decisions on the data. As such, it is very similar to a flowchart. "]},{"cell_type":"markdown","metadata":{"id":"brA0IUl2aU3L"},"source":["In this notebook we explore a simple decision tree and take a closer look at random forests. We start with the concept of information gain, vital to random forests."]},{"cell_type":"markdown","metadata":{"id":"lP6zDXIzbFtQ"},"source":["Imagine that we have a basket of green apples, oranges, and bananas. Without examining the basket, we have very little information. To gain more information we might consider if a fruit is orange in colour or not. This will immediately split the oranges from the green apples and the bananas. We have gained information. We see a simplified decision tree analgoue in the image below."]},{"cell_type":"markdown","metadata":{"id":"YU_jfeSYA58H"},"source":["<img src=\"https://drive.google.com/uc?id=1y7q1noKhje77gr7NJuXFguSrIlUOKyhc\" width=600>"]},{"cell_type":"markdown","metadata":{"id":"C7nm7sOqBlt5"},"source":["As the image shows, a decision tree asks questions at each __node__. The first question is the __root node__. All nodes that follow from a previous node are __child nodes__ and the node from which it originated is a __parent node__. The last nodes are also termed __leaf nodes__ or __terminal nodes__. A __branch__ is any tree structure that _flows from_ a parent node. The __depth__ of a tree is longest path from the root node to a leaf. In the image above the depth is $2$ (there are two layers below the root node on the right)."]},{"cell_type":"markdown","metadata":{"id":"nTMkseW-S8BZ"},"source":["In our image above, the leaf nodes are __pure__. They only contain a single class. We have gained information by _asking our questions_ and dividing the data set. We will se later that there are ways of calculating information gain."]},{"cell_type":"markdown","metadata":{"id":"V8i7tU5pB5ru"},"source":["Different questions could be asked of the data leading to different trees. In the image above, one of the feature variables (weight) was not even included."]},{"cell_type":"markdown","metadata":{"id":"iT2UFamNCsQq"},"source":["Many trees can be generated together in an ensemble of trees. This leads to random forests and gradient boosted trees. Such algorithms can greatly improve on a simple decision tree."]},{"cell_type":"markdown","metadata":{"id":"OGXgLrzTRWvC"},"source":["## PACKAGES USED IN THIS NOTEBOOK"]},{"cell_type":"code","metadata":{"id":"BF2Bu73yQDhS"},"source":["# The usual suspects\n","import numpy as np\n","import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1DFdVVGdRfQ4"},"source":["# The industry-leading scikit-learn package for machine learning in Python\n","from sklearn.datasets import make_regression\n","from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_graphviz\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n","from sklearn import metrics\n","from sklearn.preprocessing import LabelEncoder, LabelBinarizer\n","import pydotplus\n","from IPython.display import Image"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KN4Yw5-CRk_y"},"source":["# Data visualisation\n","import plotly.graph_objects as go\n","import plotly.express as px\n","import plotly.figure_factory as ff\n","import plotly.io as pio\n","pio.templates.default = 'plotly_white'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"26mlJddoRprr"},"source":["# Two more data visualisation packages\n","import matplotlib.pyplot as plt\n","import seaborn as sns"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jIrxQEdtRs2T"},"source":["%config InlineBackend.figure_format = \"retina\" # For Retina type displays"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rfhgp4xrRu1e"},"source":["# Format tables printed to the screen (don't put this on the same line as the code)\n","%load_ext google.colab.data_table"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lGjC8RLP9v0q"},"source":["## DECISION TREES"]},{"cell_type":"markdown","metadata":{"id":"q7CBFt9T906Y"},"source":["The knowledge we require to use random forests starts by understanding a decision tree. Below, we see a dataset with three categorical feature variables, each with three elements in its sample space. The target variable is dichotomous."]},{"cell_type":"code","metadata":{"id":"3B5wDmaV-R6m"},"source":["cat_1 = ['I', 'I', 'I', 'I', 'I', 'I', 'II', 'II', 'II', 'III', 'III', 'I', 'I', 'I', 'I', 'III', 'III', 'III', 'III', 'III', 'III']\n","cat_2 = ['A', 'A', 'A', 'B', 'C', 'C', 'A', 'A', 'B', 'B', 'C', 'A', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'C']\n","cat_3 = ['2', '2', '1', '2', '1', '1', '2', '1', '2', '1', '1', '2', '2', '2', '3', '3', '2', '3', '3', '2', '3']\n","target = ['No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes']\n","\n","df = pd.DataFrame({\n","    'CAT1':cat_1,\n","    'CAT2':cat_2,\n","    'CAT3':cat_3,\n","    'Target':target\n","})\n","\n","df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OS-UdQLKAgFT"},"source":["We can view the frequency of each variable's sample space elements."]},{"cell_type":"code","metadata":{"id":"MD4TVfAZASno"},"source":["df.CAT1.value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ylGCbl5JAXKP"},"source":["df.CAT2.value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"meZdcacHAa62"},"source":["df.CAT3.value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c5SoH4u8AdO9"},"source":["df.Target.value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CcivjU2qAaYq"},"source":["A decision tree is similar to a flowchart. Our aim is to build a decision tree to predict the target class."]},{"cell_type":"markdown","metadata":{"id":"cOdJ55wBA3Dq"},"source":["We can make our root node any of the three feature variables. We shall start with the first categorical variable. Using the `groupby` method, we can see the proportion of target classes in each child node. There are three child nodes that follow from this root node as there are three sample space elements in the `CAT1` variable."]},{"cell_type":"code","metadata":{"id":"aQLZG-2XBsv5"},"source":["df.groupby('CAT1')['Target'].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wjx3XeKwFUU2"},"source":["The image below gives a visual representation of the results following from using `CAT1` as the root node."]},{"cell_type":"markdown","metadata":{"id":"2Ly381c_FGs1"},"source":["<img src=\"https://drive.google.com/uc?id=117S_JJPZGnvraN3FoYbPs1Bim-sKARvA\">"]},{"cell_type":"markdown","metadata":{"id":"KT35nT4_FfdM"},"source":["We have been introduced to the term _pure node_. This means that there is also an _impure node_. __Purity__ refers to class frequency. If a child node only contains a single class it is pure (as with the second child node), else it is impure."]},{"cell_type":"markdown","metadata":{"id":"0uibd8hAF8pC"},"source":["With our aim that our decision three is knowing the target class, we already know that if we choose `CAT1` as our root node that a value of `II` will always predict a `No` for the target class. It is now a leaf or terminal node. Not so for the other two nodes (`CAT1` values of `I` and `III`). We need for them to _branch_ further."]},{"cell_type":"markdown","metadata":{"id":"ZFvzZYb_P7lc"},"source":["If we select `CAT2` for the firts child node on the left (`CAT1` being `I`) we get the following results (using `groupby` again, after selecting `I`)."]},{"cell_type":"code","metadata":{"id":"1dvDyNx8FGW-"},"source":["df.loc[df.CAT1 == 'I'].groupby('CAT2')['Target'].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y2jV5rLOQXii"},"source":["Now we see that for values of `C` we get a pure node. So, if `CAT1` is `I` and then `CAT2` is `C` then our decision tree predicts a target class of `No`. What about the other child node (`III`)? Below, we also choose `CAT2` for it."]},{"cell_type":"code","metadata":{"id":"uzpclsq4Qycx"},"source":["df.loc[df.CAT1 == 'III'].groupby('CAT2')['Target'].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Va0W2boJQ4VD"},"source":["`CAT2` brings no pure nodes. So what if we choose `CAT3` instead?"]},{"cell_type":"code","metadata":{"id":"F4gd7OqDRB1D"},"source":["df.loc[df.CAT1 == 'III'].groupby('CAT3')['Target'].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CWLXkZxRRFqX"},"source":["All three child nodes are pure. "]},{"cell_type":"markdown","metadata":{"id":"D5vTMJUYRU8w"},"source":["We could carry on this process until all nodes are pure. This random selection might be very inefficient and the depth might be quite large. So, how do we improve on our selection of variables for our nodes? The answer is information gain."]},{"cell_type":"markdown","metadata":{"id":"9VTOjtyJD9jB"},"source":["## INFORMATION GAIN"]},{"cell_type":"markdown","metadata":{"id":"pBR_wNEMD_nh"},"source":["We have seen that any variable can be chosen at a node. Given the data, a decision tree must decide on these variables."]},{"cell_type":"markdown","metadata":{"id":"qJQ07tPIEU_3"},"source":["This decision is made using __information gain__. We require maximum information gain at each node. Information gain is the difference in information before and after a node. An equation for information gain in showed in (1)."]},{"cell_type":"markdown","metadata":{"id":"Ds76BnGcEs8A"},"source":["$$\\text{IG} \\left( \\text{D}_{p} , \\text{f} \\right) = \\text{I} \\left( \\text{D}_{p} \\right) - \\sum_{i=1}^{m}{\\frac{N_{i}}{N} \\text{I} \\left( \\text{D}_{i} \\right)} \\tag{1}$$"]},{"cell_type":"markdown","metadata":{"id":"MT4JqRI4Feh1"},"source":["Here $\\text{IG}$ is information gain given the data set of a parent node, $\\text{D}_p$, and the feature, $\\text{f}$. $\\text{I}$ is an impurity criterion (see below). $N$ is the total number of samples and $\\text{D}_{i}$ is the data set of the $i^{\\text{th}}$ child node. The equation simply states that we subtract the averarge information from child nodes from that of their parent node."]},{"cell_type":"markdown","metadata":{"id":"fJDimMxuGTWD"},"source":["Two commonly used impurity criteria are the entropy and Gini index, shown in (2) and (3)."]},{"cell_type":"markdown","metadata":{"id":"XahpFT17Gepc"},"source":["$$ \\text{I}_{\\text{Entropy}} = - \\sum_{i=1}^{c}{p_{i} \\log_{2} \\left( p_{i} \\right)} \\tag{2}$$"]},{"cell_type":"markdown","metadata":{"id":"O98i8pHBGew4"},"source":["$$\\text{I}_{\\text{Gini}} = 1 - \\sum_{i=1}^{c}{p_{i}^{2}} \\tag{3}$$"]},{"cell_type":"markdown","metadata":{"id":"jMGEc7aFGtos"},"source":["Gini impurity can only be used for classification problems (categorical target variable). Here, $p_{i}$ is the proportions of observations that belongs to class $c$ for a particular node."]},{"cell_type":"markdown","metadata":{"id":"X978LTM0hRFb"},"source":["We will discuss entropy is more detail. It requires us to understand the logarithm function and summation notation."]},{"cell_type":"markdown","metadata":{"id":"C9TTwpgjXO0e"},"source":["As a quick reminder of the logarithm we have (4)."]},{"cell_type":"markdown","metadata":{"id":"qRi-QfzBXSTI"},"source":["$$y = \\log_{2} \\left( x \\right) \\text{ means } 2^{y} = x \\tag{4}$$"]},{"cell_type":"markdown","metadata":{"id":"-MBJjkncXldc"},"source":["The $y$ (the solution we seek) is what we have to raise the base ($2$ is this case) to, to get $x$."]},{"cell_type":"markdown","metadata":{"id":"MtvEe4ElX2GD"},"source":["$\\Sigma$ is the summation symbol. It has a subscript and a superscript. The former tells us where to start counting and the latter is where we stop. The increment is $1$. In (5) we get a look at how summation notation works for adding three numbers denoted as $x_{1}$, $x_{2}$, and $x_{3}$."]},{"cell_type":"markdown","metadata":{"id":"gtR_UKCsYWhm"},"source":["$$\\sum_{i=1}^{3} \\left( x_{i} \\right) = x_{1} + x_{2} + x_{3} \\tag{5}$$"]},{"cell_type":"markdown","metadata":{"id":"cf4_m048YigR"},"source":["We simply increment the value of $i$ at each step."]},{"cell_type":"markdown","metadata":{"id":"MVr3Uhi0Swkl"},"source":["Back to our equation for entropy, (2). __Shannon entropy__ is a measure of information. When we only have the data set and have not constructed a decision tree, our entropy (a measure of missing information) is high and our information is low. We need to gain information and decrease entropy (decrease the amount of missing knowledge about our target in this case). To understand this equation, we view our example from above."]},{"cell_type":"markdown","metadata":{"id":"Q_kMPZ_hTopN"},"source":["We have two target classes, so $i$ starts at $1$ and goes to $c=2$. From this we have $p_{1}$, the probability of say ,`Yes` (we are free to choose), as the number of `Yes` classes at the first child node (`I`) divided by the total number of observations (`Yes` + `No`) of $10$. So $p_{1}$ is $4$ divided by $10$. For $i=2$, that is to say `No`, $p_{2}$ would be $6$ divided by $10$. The $\\log_{2}$ is the logarithm base $2$. For clarity, we have (6) that shows the entropy for the first child node in various ways. To remind us of the child nodes of the root node, we repeat the grouping again below."]},{"cell_type":"code","metadata":{"id":"2esNbMGaWMcP"},"source":["df.groupby('CAT1')['Target'].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aEKaHxIIVRF0"},"source":["$$ \\begin{align} &\\text{I}_{\\text{I}} = - {p_{1} \\log_{2} p_{1}} - {p_{2} \\log_{2} p_{2}} \\\\ &\\text{I}_{\\text{I}} = - p_{\\text{Yes}} \\log_{2} p_{\\text{Yes}} - p_{\\text{No}} \\log_{2} p_{\\text{No}} \\\\ &\\text{I}_{\\text{I}} = -\\frac{4}{10} \\log_{2} \\frac{4}{10} - \\frac{6}{10} \\log_{2} \\frac{6}{10} \\end{align} \\tag{6}$$"]},{"cell_type":"markdown","metadata":{"id":"0FvO2tCjXAU3"},"source":["The numpy `log2` function calculates the logarithm base $2$."]},{"cell_type":"code","metadata":{"id":"D8aagDFVW0Wc"},"source":["cat1_I = -((4/10) * (np.log2(4/10))) - ((6/10) * (np.log2(6/10)))\n","cat1_I"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4GZmFhShY9z9"},"source":["Below, we do this for the other two child nodes in the image above. Remember that in the second node (`CAT1` = `II`) we have a pure node of three `No` classes. In the third node (`CAT1` = `III`) we have six `Yes` classes and two `No` classes."]},{"cell_type":"markdown","metadata":{"id":"TOlkpbk_ZmdI"},"source":["Since the logarithm of $0$ is not defined, we do not include it in the equation."]},{"cell_type":"code","metadata":{"id":"OHJFEQ0IZblV"},"source":["# Second node\n","cat1_II = - ((3/3) * (np.log2(3/3)))\n","cat1_II"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"asZFRVlQZvXb"},"source":["The result is $0$ (bar the rounding error)."]},{"cell_type":"code","metadata":{"id":"KDZIy82kZDXz"},"source":["# Third node\n","cat1_III = -((6/8) * (np.log2(6/8))) - ((2/8) * (np.log2(2/8)))\n","cat1_III"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uNUdwQM5aAfU"},"source":["Entropy ranges from $0$ where we have complete information (a pure node) to $1$ where we have no information."]},{"cell_type":"markdown","metadata":{"id":"Nu-aqpeFaKYO"},"source":["We also look at the entropy of the parent node. At the root we simply have the frequency of the target classes."]},{"cell_type":"code","metadata":{"id":"vZv3gGvcaXNN"},"source":["df.Target.value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aCE4YaIxahmj"},"source":["# Root node\n","start = -((10/21) * (np.log2(10/21))) - ((11/21) * (np.log2(11/21)))\n","start"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MF9RV9N_atGG"},"source":["If we average over the entropy of each of the child nodes and subtract this average from the entropy of the root (parent) node, we know the information gain for choosing `CAT1` as our root node. This is the equation (1) above."]},{"cell_type":"code","metadata":{"id":"VsHSlKiBbWNg"},"source":["# Information gain given CAT1 as choice for root node\n","start - np.mean([cat1_I, cat1_II, cat1_III])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5XQrYg5ZbmJj"},"source":["Would it have been better to choose one of the other variables? We start by taking a look at the information gain from `CAT2` as root node."]},{"cell_type":"code","metadata":{"id":"iKUz4q4DbzRY"},"source":["df.groupby('CAT2').Target.value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qMbZknu4b6tK"},"source":["# Calculating the three entropies\n","cat2_A = -((1/6) * np.log(1/6)) - ((5/6) * np.log(5/6))\n","cat2_B = -((8/11) * np.log(8/11)) - ((3/11) * np.log(3/11))\n","cat2_C = -((1/4) * np.log(1/4)) - ((3/4) * np.log(3/4))\n","\n","start - np.mean([cat2_A, cat2_B, cat2_C])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kUOr4O_hcent"},"source":["The information gain is higher. What about `CAT3`?"]},{"cell_type":"code","metadata":{"id":"BN6KAQGHcpSS"},"source":["df.groupby('CAT3').Target.value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MOk94Py8ckef"},"source":["# Calculating the three entropies\n","cat3_1 = - ((6/6) * np.log(6/6))\n","cat3_2 = -((5/10) * np.log(5/10)) - ((5/10) * np.log(5/10))\n","cat3_3 = -((5/5) * np.log(5/5))\n","\n","start - np.mean([cat3_1, cat3_2, cat3_3])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zbbyr7tIc9QC"},"source":["The information gain is even higher. This would be the best choice for our first node."]},{"cell_type":"markdown","metadata":{"id":"uc_-k7dBdGiS"},"source":["This is one algorithm used by a decsion tree. It repeats this process at every branch until it reaches purity in all child nodes or until a hyperparameter setting requires it to stop branching (see later)."]},{"cell_type":"markdown","metadata":{"id":"eOrl6riihiXk"},"source":["When can and should a decision tree stop? One obvious stopping criterium is when all the child nodes are leaves or terminal nodes, i.e. they are pure. This is a problematic approach as the depth can be large and the model will probably overfit the training data and not generalise well to unseen data."]},{"cell_type":"markdown","metadata":{"id":"HcwjXfcBh6Zj"},"source":["In another method, we set a minimum information gain. Once successive branching fails to improve beyond this minimim, the decision tree terminates. We can also call a halt when a number of the child nodes contain less than a set proportion of the classes."]},{"cell_type":"markdown","metadata":{"id":"mgUvwauUjULn"},"source":["While a single decision tree is relatively easy to create and understand, it does have drawbacks. We have mentioned overfitting. This is worsened by smaller data sets. __Pruning__ is a technique where the depth is made more shallow. This can be set or occur after a tree is fully constructed. Such pruned trees might do better on unseen data."]},{"cell_type":"markdown","metadata":{"id":"lqZWHq8ajxJC"},"source":["Another major drawback occurs when some feature variables contain many classes. A tree might preferentially split on this variable. __Information gain ratio__ reduces the bias a tree has for these variables by looking at the size and number of branches of each variable."]},{"cell_type":"markdown","metadata":{"id":"FQVbaMnnkSHN"},"source":["In the next section, we use the `DecisionTreeClassifier` from the scikit-learn package to investigate our data set."]},{"cell_type":"markdown","metadata":{"id":"GB97uPTNdk89"},"source":["## A DECISION TREE CLASSIFIER"]},{"cell_type":"markdown","metadata":{"id":"U9SfRDDTdp78"},"source":["The scikit-learn package provides a decision tree classifier for classification problems. We can use it on our simple data set. The process is as with the $k$ nearest neighbour classifier from the previous notebook. First, we instantiate the classifier and then fit the data to it."]},{"cell_type":"markdown","metadata":{"id":"Kz96HAI7eMoS"},"source":["First, though, we have to transform our data. The `DecisionTreeClassifier` class only works with numerical data. We use the `LabelEncoder` and `LabelBinarizer` to transcode our variable values."]},{"cell_type":"code","metadata":{"id":"hPsXDW5GmW0l"},"source":["# Instantiate the label encoder\n","label_encoder = LabelEncoder()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hu70gVOlmgmI"},"source":["# Instantiate the label binazier\n","label_binarizer = LabelBinarizer()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UdV6MrZ-kx_A"},"source":["The `fit_transform` method for each of the encoders will fit and transform the data."]},{"cell_type":"code","metadata":{"id":"PhUXZf_gms_9"},"source":["encoded_cat1 = label_encoder.fit_transform(cat_1)\n","encoded_cat2 = label_encoder.fit_transform(cat_2)\n","encoded_cat3 = label_encoder.fit_transform(cat_3)\n","y = label_binarizer.fit_transform(target).flatten()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xMP27iHSk7KB"},"source":["Now we create a numpy array and append the three feature variables."]},{"cell_type":"code","metadata":{"id":"-rTrXZUqeRCo"},"source":["X = []\n","\n","for i in range(len(y)):\n","  X.append([encoded_cat1[i], encoded_cat2[i], encoded_cat3[i]])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F-iuhCdylCGE"},"source":["All that remains is to instantiate our classifier and fit the data."]},{"cell_type":"code","metadata":{"id":"OyhclTU3g_x-"},"source":["# Instantiate the classifier\n","d_tree = DecisionTreeClassifier(criterion='entropy')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uAePlVOvhMjp"},"source":["# Fit the data (in numpy array format)\n","d_tree.fit(\n","    X,\n","    y\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I2ybOjBapMmb"},"source":["If you are running this notebook on a local system then the following code will export a PNG image of the decision tree.\n","\n","```\n","feature_names = ['Cat 1', 'Cat 2', 'Cat 3']\n","target_names = ['No', 'Yes']\n","\n","dot_data = export_graphviz(\n","    d_tree,\n","    out_file=None,\n","    class_names=target_names\n",")\n","\n","graph = pydotplus.graph_from_dot_data(dot_data)\n","\n","Image(graph.create_png)\n","graph.write_png('tree.png')\n","```"]},{"cell_type":"markdown","metadata":{"id":"1TAbOq6zpxxq"},"source":["We can use the `predict` method to pass an unseen observation to the model."]},{"cell_type":"code","metadata":{"id":"pSGLtZSRpiea"},"source":["d_tree.predict([[1, 2, 1]])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LO0COLz8lPcJ"},"source":["We can compute the accuracy of our model by passing the feature variable array to the `predict` method."]},{"cell_type":"code","metadata":{"id":"JslFDgWRqK7A"},"source":["y_pred = d_tree.predict(X)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Rv14lItylbKI"},"source":["We use logic to return `True` and `False` values while comparing the predicted and the true target variable values. Since `True` is represented internally as a $1$, we can sum over all the Boolean values and divide by the number of observations to return the accuracy of the model."]},{"cell_type":"code","metadata":{"id":"pd0c-FRDqWH9"},"source":["np.sum(y == y_pred) / len(y_pred)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6CT00nzc5bWo"},"source":["As with the $k$ nearest neighbour classifier, we can use a confusion matrix plot to evaluate the model's prediction using the test data predictions and actual values."]},{"cell_type":"code","metadata":{"id":"u6gCQIJ54Br_"},"source":["metrics.plot_confusion_matrix(d_tree,\n","                              X,\n","                              y);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lofkMdf-X5S9"},"source":["From this we can use all the other metrics described in the previous notebook."]},{"cell_type":"markdown","metadata":{"id":"4CWm8Fwhrke8"},"source":["## A DECISION TREE REGRESSOR"]},{"cell_type":"markdown","metadata":{"id":"yh5wfV5FrnqW"},"source":["The scikit-learn decision tree regressor class is very simular to the classifier class. In regression problems, the target variable is continuous numerical."]},{"cell_type":"markdown","metadata":{"id":"eEVCk5KuryT1"},"source":["To work through an example of a decision tree regression problem, we generate data using the `make_regression` function from the models module of the scikit-learn package."]},{"cell_type":"code","metadata":{"id":"v0io_GuDsDIs"},"source":["X, y = make_regression(\n","    n_samples=1000, # Sample size\n","    n_features=4, # Total number of feature variables\n","    n_informative=2, # Number of feature variable that are correlated to target\n","    noise=0.9, # Add noise to the data\n","    random_state=12 # For reproducible results\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NDjxca96tE-p"},"source":["To visualise the correlation between every pair of variables we create a scatter plot matrix after importing the data into a pandas DataFrame object."]},{"cell_type":"code","metadata":{"id":"N1Cd-1_qtNwH"},"source":["columns = ['Var1', 'Var2', 'Var3', 'Var4'] # Feature variable names\n","\n","regression_data = pd.DataFrame(\n","    X,\n","    columns=columns\n",")\n","\n","regression_data['Target'] = y # Add target variable as another column\n","\n","regression_data[:5] # First five observations"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TjOYCdWJtnp0"},"source":["px.scatter_matrix(\n","    regression_data,\n","    title='Scatter plot matrix'\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KSC9NSgbtw6h"},"source":["We note that `Var1` and `Var3` seem to be correlated to the target variable."]},{"cell_type":"markdown","metadata":{"id":"Y8PUl1wqt3Pd"},"source":["Below, we instantiate the regressor class and then proceed as we did above with the classification problem."]},{"cell_type":"code","metadata":{"id":"VgoYs_h2t-tS"},"source":["# Instantiate the decision tree regressor class\n","regressor = DecisionTreeRegressor() # All hyperparameters left at their default values"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qlsu34X0u0ZU"},"source":["We take the added step of splitting the data into a training and a test set."]},{"cell_type":"code","metadata":{"id":"_PWNQS95ugq_"},"source":["x_train, x_test, y_train, y_test = train_test_split(\n","    X,\n","    y,\n","    test_size=0.2,\n","    random_state=12\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VcptpJ8DvRmk"},"source":["x_train.shape, y_train.shape # Verifying the splitting result"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WrpqOebPu4yq"},"source":["Now we can fit the model and evaluate its performance."]},{"cell_type":"code","metadata":{"id":"uaVhiesju8J8"},"source":["dt_reg_model = regressor.fit(\n","    x_train,\n","    y_train\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uQyvls7hwDCc"},"source":["We can use the coefficent of determination, $R^{2}$, to evaluate the model given the test set."]},{"cell_type":"code","metadata":{"id":"37oY2YNDvwEN"},"source":["dt_reg_model.score(\n","    x_test,\n","    y_test\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"13REFtmsxY5p"},"source":["We can calculate the predicted target values using the `predict` method."]},{"cell_type":"code","metadata":{"id":"ZhvjG7-3wTpd"},"source":["y_reg_pred = dt_reg_model.predict(\n","    x_test\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_lcJRYkOxhbL"},"source":["A scatter plot shows very good correlation between the actual and predicted target values."]},{"cell_type":"code","metadata":{"id":"jdThs7Jhwwou"},"source":["go.Figure(\n","    go.Scatter(\n","        x=y_test,\n","        y=y_reg_pred,\n","        mode='markers',\n","        marker={\n","            'size':10\n","        }\n","    )\n",").update_layout(\n","    title='Actual vs predicted values for the test set',\n","    xaxis={'title':'Actual target values'},\n","    yaxis={'title':'Predicted targte values'}\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AMxSz1EsxpqR"},"source":["## RANDOM FORESTS"]},{"cell_type":"markdown","metadata":{"id":"T99Dtgnnxruj"},"source":["A single decision tree is easy to understand and to generate. It does not fare very well in the real world. To improve on the performance, we use ensemble techniques such a random forests. As the name suggests, it is a collection of trees."]},{"cell_type":"markdown","metadata":{"id":"AIm48to8yBn7"},"source":["The decision trees in a random forest are all individual models and the final result is majority vote or an average of all the predictions."]},{"cell_type":"markdown","metadata":{"id":"kC8KqFvvyNu6"},"source":["The trees themselves select a random set of the feature variables, a random sample of the observations (resampling with replacement), and are trained to various depths. These are all hyperpaarmeters that can be set. This combination can improve the performance on real-world data."]},{"cell_type":"markdown","metadata":{"id":"Gs2ZUwKXzdGQ"},"source":["As an example, we will use the same data as with the decision tree regression problem above. The steps we take to generate, train, and evaluate the model should now be very familiar."]},{"cell_type":"code","metadata":{"id":"c8Pn0Er6zZ3C"},"source":["rf_regressor = RandomForestRegressor() # Hyperparameters are left at their default"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tela1DPk0fZ8"},"source":["# Training the model\n","rf_reg_model = rf_regressor.fit(\n","    x_train,\n","    y_train\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9TPBGsPj0-6Y"},"source":["# Coefficent of correlation\n","rf_reg_model.score(\n","    x_test,\n","    y_test\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1GANfd7l8Clt"},"source":["This is almost $1.0$. Below, we look at a scatter plot of the actual versus the predicted values."]},{"cell_type":"code","metadata":{"id":"_A0kF-3q8B3H"},"source":["y_reg_pred = dt_reg_model.predict(\n","    x_test\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7mvV51WG8UCk"},"source":["go.Figure(\n","    go.Scatter(\n","        x=y_test,\n","        y=y_reg_pred,\n","        mode='markers',\n","        marker={\n","            'size':10\n","        }\n","    )\n",").update_layout(\n","    title='Actual vs predicted values for the test set',\n","    xaxis={'title':'Actual target values'},\n","    yaxis={'title':'Predicted targte values'}\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F-4wV8zScKmJ"},"source":["## TENSORFLOW DECISION FORESTS"]},{"cell_type":"markdown","metadata":{"id":"hWtNKklbcNj3"},"source":["Google, which provides the popular TensorFlow deep neural network architecture, now also provides a wrapper for the Yggdrasil Decision Forest C++ libraries named `tensorflow_decision_forests`. The name is a bit different from the traditional decion tree and random forest in that it combines both terms."]},{"cell_type":"markdown","metadata":{"id":"dWVZcvLW3F8-"},"source":["The tensorflow_decision_forests package can use numerical and categorical variables. We do not need to transform the data, i.e. standardize the numerical variables or convert categorical variables into numerical variables, except the target variable as it is used by the keras module of this package for metrics (see later). It can also manage missing data."]},{"cell_type":"markdown","metadata":{"id":"ykFimsXletnZ"},"source":["### INSTALLING AND IMPORTING THE PACKAGE"]},{"cell_type":"markdown","metadata":{"id":"MOBP5eqBcsmS"},"source":["At the time of the creation of this notebook, it is not yet part of Colab. We have to install it first."]},{"cell_type":"code","metadata":{"id":"9IJ-vjNVczPA"},"source":["# Install this package\n","!pip install tensorflow_decision_forests"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gupgE5VtdCW7"},"source":["We can now import the package, as well as some other packages we will need."]},{"cell_type":"code","metadata":{"id":"BtbcMq_IRyAX"},"source":["import tensorflow_decision_forests as tfdf\n","import tensorflow as tf"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dFXbD-WteWM0"},"source":["The tensorflow_decision_forests package is part of the TensorFlow family. Always check what the current version is for updates and changes."]},{"cell_type":"code","metadata":{"id":"zcZObPHPcrr1"},"source":["# Current version\n","tfdf.__version__"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PbVymuaQel-B"},"source":["### LOADING A DATA SET"]},{"cell_type":"markdown","metadata":{"id":"Vu151icne7kU"},"source":["Along with the official tutorials by Google, we import the very famous Palmer penguins ML data set directly from the internet."]},{"cell_type":"code","metadata":{"id":"KfSz44_HdzcU"},"source":["# Download the data set\n","!wget -q https://storage.googleapis.com/download.tensorflow.org/data/palmer_penguins/penguins.csv -O /tmp/penguins.csv"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ttBP2KkjfLgi"},"source":["Colab saves this as a temporary file that we can import using pandas."]},{"cell_type":"code","metadata":{"id":"7xWKyqEIfJkZ"},"source":["penguins = pd.read_csv('/tmp/penguins.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jGgS4Hx0fcm0"},"source":["Below, we inspect the data set."]},{"cell_type":"code","metadata":{"id":"jhVL4iqgfSC6"},"source":["penguins.shape # Number of observations and variables"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F_c_wtpKf7_I"},"source":["This is a small data set with only $344$ observations. There are eight variables."]},{"cell_type":"code","metadata":{"id":"9B33WwPlfn6w"},"source":["penguins.info() # Variable data type and missing data information"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bydtkbNAf00x"},"source":["We note a few missing values, especially for the `sex` variable."]},{"cell_type":"code","metadata":{"id":"6s6Ge9SifrkL"},"source":["penguins[:5] # First five observations"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"toavtZF2gMrL"},"source":["There are three penguin species, with underrepresentation of the Chinstrap species."]},{"cell_type":"code","metadata":{"id":"rWNfvlpOgG7l"},"source":["penguins.species.value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D5gg1TrvoJs1"},"source":["In order to use the metrics, we need to transform the target variable to an integer data type."]},{"cell_type":"code","metadata":{"id":"lMAQUVj1gXpn"},"source":["classes = penguins.species.unique().tolist() # A list of the three species\n","classes"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xRajFWPvo-uP"},"source":["The `map` method and the `classes` list index is used to convert Adelie to $0$, Gentoo to $1$, and Chinstrap to $2$."]},{"cell_type":"code","metadata":{"id":"eMAH7q5hom1Q"},"source":["penguins.species = penguins.species.map(classes.index)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jFzghgqepbZp"},"source":["### DATA SPLITTING"]},{"cell_type":"markdown","metadata":{"id":"ypx6G_bjpjM4"},"source":["Instead of using the train test split method from the scikit-learn package, we generate a function to split the data. We call the function `split`. It takes two arguments, `ds` from the DataFrame object, and `r` for the fraction of test set values. We set the default at $0.3$ or $30$% of the observations."]},{"cell_type":"markdown","metadata":{"id":"UvVqyFa2qUHG"},"source":["Internal to the function we create a computer variable, `test_ind` to hold index values. To it we assign `True` and `False`. The effect is seen in the two code cells below."]},{"cell_type":"markdown","metadata":{"id":"FwlP2xISq6Z9"},"source":["The indices of the `True` values are used to generate the training set and for `false`, the test set."]},{"cell_type":"code","metadata":{"id":"-TWHWqcAoxpm"},"source":["def split(ds, r=0.3):\n","  test_ind = np.random.rand(len(ds)) < r\n","\n","  return ds[~test_ind], ds[test_ind]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DmFATzfAp_fR"},"source":["# Splitting the data\n","np.random.seed(12)\n","penguins_train, penguins_test = split(penguins)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7BJuoESLrSYB"},"source":["We investiagte the number of observations in each set using the `shape` attribute of each DataFrame object to ensure that our split was executed correctly."]},{"cell_type":"code","metadata":{"id":"IOFOBGeUrNv-"},"source":["penguins_train.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w5vOA5rurXd6"},"source":["penguins_test.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DeqX0K-BsCAq"},"source":["We also need to make sure that we have proper representation of the target classes in each set."]},{"cell_type":"code","metadata":{"id":"5H0YmWiwrZJe"},"source":["px.bar(\n","    penguins_train,\n","    x='species',\n","    title='Training set target class frequency',\n","    labels={\n","        'species':'Species'\n","    }\n",").update_xaxes(\n","    type='category'\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-QnAW0SkroWt"},"source":["px.bar(\n","    penguins_test,\n","    x='species',\n","    title='Test set target class frequency',\n","    labels={\n","        'species':'Species'\n","    }\n",").update_xaxes(\n","    type='category'\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8YQH0n1bwopH"},"source":["Finally, we need to transform the pandas dataframe objects to TensorFlow dataset objects using the `pd_dataframe_to_tf_dataset` function."]},{"cell_type":"code","metadata":{"id":"BFiTVzo9sKoW"},"source":["penguins_train = tfdf.keras.pd_dataframe_to_tf_dataset(\n","    penguins_train,\n","    label='species'\n",")\n","\n","penguins_test = tfdf.keras.pd_dataframe_to_tf_dataset(\n","    penguins_test,\n","    label='species'\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zfKQxGmj1gGH"},"source":["### CREATING A DECISION FOREST MODEL"]},{"cell_type":"markdown","metadata":{"id":"P6uKr7M71we2"},"source":["Below, we instantiate a random forest model, with default hyperparameter values."]},{"cell_type":"code","metadata":{"id":"41juWz8V1Wa5"},"source":["rf_model = tfdf.keras.RandomForestModel()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6UQPEjrd2H5D"},"source":["We set accuracy as metric and compile the model. This step is only required if we want to specify metrics."]},{"cell_type":"code","metadata":{"id":"-l598m6B2MNM"},"source":["rf_model.compile(\n","    metrics=['accuracy']\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PqR_kzRG2SE7"},"source":["### TRAINING THE MODEL"]},{"cell_type":"markdown","metadata":{"id":"iZclHbj32FE0"},"source":["Now we fit the data to the model."]},{"cell_type":"code","metadata":{"id":"rbpVx6qG2VFL"},"source":["rf_model.fit(\n","    x=penguins_train\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6QdrYLeU22zr"},"source":["The `summary` function provides information about the model."]},{"cell_type":"code","metadata":{"id":"Co9uYgr72BcG"},"source":["print(rf_model.summary())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RBBcodo93tRg"},"source":["### EVALUATING THE MODEL"]},{"cell_type":"markdown","metadata":{"id":"-As5F_K73wwt"},"source":["The `evaluate` method provides a loss and an accuracy value. Below, we evaluate the model using the test set."]},{"cell_type":"code","metadata":{"id":"pybZpAGd2j89"},"source":["evaluation = rf_model.evaluate(\n","    penguins_test,\n","    return_dict=True # Returning a dictionary of metrics\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g_0U7jxl4cCr"},"source":["We can already see the loss and the accuracy. Below, we use the `keys` and the `values` methods to return the parts of the metrics dictionary."]},{"cell_type":"code","metadata":{"id":"BLYFGGts4K4T"},"source":["evaluation.keys() # Metric dictionary keys"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"088f8_Mt4Oi9"},"source":["evaluation.values() # Metric values"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2ykuqbEW4wOf"},"source":["### VISUALISING THE MODEL"]},{"cell_type":"markdown","metadata":{"id":"YtvJVR1I42WA"},"source":["As mentioned, decision trees and random forests are interpretable. Plotting the model shows us how information was gained."]},{"cell_type":"code","metadata":{"id":"_J_2b0jb4aoO"},"source":["tfdf.model_plotter.plot_model_in_colab(\n","    rf_model,\n","    tree_idx=0,\n","    max_depth=4\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jl8GE-4k5jbS"},"source":["The model chose `flipper_length_mm` as the first node and split on whether the length was equal to or more than $207$. The first four decision node layers are shown."]},{"cell_type":"markdown","metadata":{"id":"WcfO1-vZd9OP"},"source":["We can inspect the feature variable importance using the `variable_importance` method."]},{"cell_type":"code","metadata":{"id":"k3dmnY9H5H5b"},"source":["rf_model.make_inspector().variable_importances()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yNz6c21HeDHd"},"source":["The model can also provide a self-assessment. The `evaluation` method returns the number of samples and the accuraccy."]},{"cell_type":"code","metadata":{"id":"tr-67tpe6U9B"},"source":["rf_model.make_inspector().evaluation()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x_B3tVmKag4J"},"source":["## CONCLUSION"]},{"cell_type":"markdown","metadata":{"id":"FKOKuRNKajCU"},"source":["Random forests and other ensemble techniques using decision trees have very recently gained a lot of attention. They are easier to interpret and perform better than state of the art deep neural networks in many cases."]},{"cell_type":"code","metadata":{"id":"MPSfdKUQdzZN"},"source":[""],"execution_count":null,"outputs":[]}]}