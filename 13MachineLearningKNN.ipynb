{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"13MachineLearningKNN.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"IBIe7fv0P3e8"},"source":["## MACHINE LEARNING WITH $k$ NEAREST NEIGHBOUR ALGORITHM"]},{"cell_type":"markdown","metadata":{"id":"OsC8HXB_Px7j"},"source":[">by Dr Juan H Klopper\n","\n","- Research Fellow\n","- School for Data Science and Computational Thinking\n","- Stellenbosch University"]},{"cell_type":"markdown","metadata":{"id":"twJseoAJQCq0"},"source":["## INTRODUCTION"]},{"cell_type":"markdown","metadata":{"id":"PZtOa0tSQLwa"},"source":["The $k$ nearest neighbour (kNN) algorithm is one of the easiest machine learning algorithms to understand and implement. It can be used for classification and regression problems. In the former, the target variable is categorical. In the latter, it is numerical."]},{"cell_type":"markdown","metadata":{"id":"p4kiTWsYYYA9"},"source":["In this notebook we explore the $k$ nearest neighbour machine learning (ML) algorithm. We start of with a simple example of classification before embarking on solving a more realistic problem. Along the way we will learn a lot of the basic concepts of ML. We end with a small example to help us understand how to use kNN in a regression problem."]},{"cell_type":"markdown","metadata":{"id":"pMSL6eNMhBMN"},"source":["To note upfront, for the same objects, ML uses different names than we use in statistics. Independent variables are referred to as __feature variables__ or simply __features__. The dependent variable is termed a __target variable__ or an __outcome variable__ (or simply a __target__ or an __outcome__. If the target variable is categorical, then the sample space elements are termed __classes__."]},{"cell_type":"markdown","metadata":{"id":"rhevZsSVQEaM"},"source":["## PACKAGES USED IN THIS NOTEBOOK"]},{"cell_type":"markdown","metadata":{"id":"2QZcXzOvQKN-"},"source":["The following packages will be used in this notebook."]},{"cell_type":"code","metadata":{"id":"xFykT0EYQGp4"},"source":["import numpy as np\n","from pandas import DataFrame, Series, read_csv"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ouNUEZ_ziECH"},"source":["from sklearn.datasets import make_classification\n","from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n","from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n","from sklearn import metrics\n","from sklearn.preprocessing import StandardScaler"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kOPjm0o3Rdg8"},"source":["import plotly.graph_objects as go\n","import plotly.express as px\n","import plotly.io as pio\n","pio.templates.default = 'plotly_white'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5k57aEI0m1cu"},"source":["import matplotlib.pyplot as plt\n","import seaborn as sns"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5hGIWrMdju4M"},"source":["%config InlineBackend.figure_format = \"retina\" # For Retina type displays"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ziJaXkwwjvif"},"source":["# Format tables printed to the screen (don't put this on the same line as the code)\n","%load_ext google.colab.data_table"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bv-wyUE-keMc"},"source":["from google.colab import drive  # Connect to Google Drive"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qKnsaReLQU9v"},"source":["## THE NEAREST NEIGHBOURS CONCEPT"]},{"cell_type":"markdown","metadata":{"id":"KYROXI32RmOV"},"source":["### $k$ NEAREST NEIGHBOUR CLASSIFICATION"]},{"cell_type":"markdown","metadata":{"id":"OsaC-ERXRf1M"},"source":["The $k$ nearest neighbour classifier classifies an observation based on the classes in its vicinity. Vicinity infers distance. With this ML algorithm, we measure a distance between observations."]},{"cell_type":"markdown","metadata":{"id":"iJZzmcdNR5Y5"},"source":["There are various ways to define distance. Euclidean distance (a straight line on a flat surface) is most familiar to us. We consider a single numerical variable (for a featuare variable) and two classes (for a binary target variable). The code below generates a pandas dataframe object, with two appropriately named variables."]},{"cell_type":"code","metadata":{"id":"TN6ANxJyR4sC"},"source":["np.random.seed(42)\n","\n","df = DataFrame(\n","    {'Feature':np.random.randint(10, high=20, size=7),\n","     'Target':np.random.choice(['A', 'B'], size=7)}\n",")\n","df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n8TPrBkdWZEY"},"source":["With the random seed set as $42$, we note four observations belonging to group A and three to group B."]},{"cell_type":"markdown","metadata":{"id":"N0Uu7-E4WlKK"},"source":["A scatter plot can be used to visualise this single variable for two classes. Note that two observations in group A have a feature variable value of $16$."]},{"cell_type":"code","metadata":{"id":"bn5c4LO-Sm2b"},"source":["single_dim_fig = go.Figure(\n","    go.Scatter(\n","        x=df.loc[df.Target == 'A'].Feature,\n","        y=[0, 0, 0, 0],\n","        name='A',\n","        mode='markers',\n","        marker={'size':20}\n","    )\n",").add_trace(\n","    go.Scatter(\n","        x=df.loc[df.Target == 'B'].Feature,\n","        y=[0, 0, 0],\n","        name='B',\n","        mode='markers',\n","        marker={'size':20}\n","    )\n",").update_layout(\n","    title='Variable values for two classes',\n","    xaxis={'title':'Variable value'}\n",")\n","\n","single_dim_fig"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vwQWO831W03g"},"source":["Distance, $d$, between any two values, $x_{1}$ and $x_{2}$, for this single dimension (all values are on a single axis) is given in (1). Distance is always a positive value, hence we take the absolute value of the difference for two points $x_{1}$ and $x_{2}$."]},{"cell_type":"markdown","metadata":{"id":"M-gs1u6YXR8O"},"source":["$$d = \\lvert x_{1} - x_{2} \\rvert \\tag{}$$"]},{"cell_type":"markdown","metadata":{"id":"PymTwblwXiJL"},"source":["The distance between $12$ and $19$ is therefor $\\lvert 12 - 19 \\rvert = \\lvert -7 \\rvert = 7$."]},{"cell_type":"markdown","metadata":{"id":"Vvp5jgxuXw-r"},"source":["Now we introduce an unknown observation with a value of $12.5$. The $k$ in $k$ nearest neigbours is an integer (whole number) reflecting the number of neighbours to an observation. It is set at an odd value. In this instance, we shall say $k=3$."]},{"cell_type":"code","metadata":{"id":"6Jl3HTwRZqUj"},"source":["single_dim_fig.add_trace(\n","    go.Scatter(\n","        x=[12.5],\n","        y=[0],\n","        name='Unkown class',\n","        mode='markers',\n","        marker={'size':20}\n","    )\n",")\n","\n","single_dim_fig"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ttiBsO-5ZaV_"},"source":["The three nearest neigbours are $12, 13$, and $14$. The distance to these nearest (closets) neighbours are $\\lvert 12.5 - 12 \\rvert = 0.5$, $\\lvert 12.5 - 13 \\rvert = 0.5$, and $\\lvert 12.5 - 14 \\rvert = 1.5$."]},{"cell_type":"markdown","metadata":{"id":"Jyo4YrvPZdeQ"},"source":["We note that the classes for these three nearest neigbours are group B, group B, and group A. Since we chose an odd number of neighbors, we can simply take a _majority vote_. This would be group B. The $k$ nearest neighbour classifier would therefor classify this new observation as belonging to target class `B`."]},{"cell_type":"markdown","metadata":{"id":"UJQU5A7NaN2o"},"source":["If we add another numerical variable, we can plot the data as a scatter plot in the plane. We do this below after adding another random set of values."]},{"cell_type":"code","metadata":{"id":"aI5pym1QT4tu"},"source":["np.random.seed(42)\n","\n","df['Feature2'] = np.random.randint(100, 200, 7) / 10"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jiez1Lgeaqr8"},"source":["df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vkb9izAxajX0"},"source":["two_dim_fig = go.Figure(\n","    go.Scatter(\n","        x=df.loc[df.Target == 'A'].Feature,\n","        y=df.loc[df.Target == 'A'].Feature2,\n","        name='Group A',\n","        mode='markers',\n","        marker={'size':20}\n","    )\n",").add_trace(\n","    go.Scatter(\n","        x=df.loc[df.Target == 'B'].Feature,\n","        y=df.loc[df.Target == 'B'].Feature2,\n","        name='Group B',\n","        mode='markers',\n","        marker={'size':20}\n","    )\n",").update_layout(\n","    title='Variable values for two classes',\n","    xaxis={'title':'Feature value'},\n","    yaxis={'title':'Feature 2 value'}\n",")\n","\n","two_dim_fig"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4Ntc42VkblvT"},"source":["The equation for the Euclidean distance in the plane (two dimensional space) is the Pythagorean Theorem and is shown in (2) for two points in the plane, $P_{1} = \\left( x_{1} , y_{1} \\right)$ and $P_{2} = \\left( x_{2} , y_{2} \\right)$."]},{"cell_type":"markdown","metadata":{"id":"p112cn73b8cl"},"source":["$$d \\left( P_{1} , P_{2} \\right) = \\sqrt{{\\left( x_{1} - x_{2} \\right)}^{2} + {\\left( y_{1} - y_{2}  \\right)}^{2}} \\tag{2}$$"]},{"cell_type":"markdown","metadata":{"id":"d9Xs2Ur9cRTu"},"source":["We are only interested in the positive value of the square root. Since both expressions in the square root are squared, we will always have a value of greater than or equal to $0$ and can therefor take the square root."]},{"cell_type":"markdown","metadata":{"id":"3omq4p5fcnRF"},"source":["A new observation with values `Feature` $=14$ and `Feature2` $=18$ is shown below."]},{"cell_type":"code","metadata":{"id":"yCRSd3SFbA5h"},"source":["two_dim_fig.add_trace(\n","    go.Scatter(\n","        x=[14],\n","        y=[18],\n","        name='Unkown class',\n","        mode='markers',\n","        marker={'size':20}\n","    )\n",").update_yaxes(\n","    scaleanchor='x',\n","    scaleratio=1,\n","  ) # For same x and y axis scale\n","\n","two_dim_fig"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XF-n2BrxdEG9"},"source":["The nearest $k=3$ neigbours are group A, group B, and group B. This unknown observation is therefor classified as belonging to group B. Below, we set up a function to calculate this distance and use it for the three nearest neighbours."]},{"cell_type":"code","metadata":{"id":"oYWxFYKydn2H"},"source":["def dist_2D(x1, y1, x2, y2):\n","  distance = np.sqrt((x1 - x2)**2 + (y1 - y2)**2)\n","  return distance"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lNT0jD5xdAFT"},"source":["dist_2D(14, 18, 14, 17.1) # Group B observation"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MvpNrPCzeALh"},"source":["dist_2D(14, 18, 13, 19.2) # Group A observation"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LhREu8byeFjo"},"source":["dist_2D(14, 18, 12, 18.2) # Group B observation"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_LqTQgkNfu-R"},"source":["All other observations are _further_ away."]},{"cell_type":"markdown","metadata":{"id":"b0jmStkagw7n"},"source":["The scikit-learn package has many ML algorithms including a $k$ nearest neighbour classifier (for classification problems). We will use this classifier in an example. First, though, we will use the `make_classification` function from the datasets module of scikit-learn. This function generates random value datasets for ML tasks. The code comment explains the arguments used. The documentation for this function list all the other arguments, which we will leave at their default values."]},{"cell_type":"code","metadata":{"id":"0nMyzvB2fkUe"},"source":["X, y = make_classification(\n","    n_samples=200, # Number of observations\n","    n_features=5, # Number of features\n","    n_informative=3, # Number of features that are informative as to the class\n","    n_redundant=2, # Number of redundant feautres\n","    n_classes=2, # Setting a binary target variable\n","    flip_y=0.1, # Flip 10% of the observations to the other class\n","    random_state=42 # Seeding the pseudo-random number generator\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L2RtJ1dBkFkA"},"source":["The function returns two arrays, which we have assigned to the commonly used variable `X` for the set of feature variables and `y` for the target variable. It is worthwhile to look at the type of objects assigned to these variables and their dimensions."]},{"cell_type":"code","metadata":{"id":"TjZNY-Hfi39Q"},"source":["type(X) # X is a numpy multi-dimensional array"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wYADaBrbkW9M"},"source":["X.shape # Shape attribute shows 200 observations and 5 variables"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jEvcpaHhkzyL"},"source":["X.dtype # Values are 64-bit floating point values (decimals)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oWkgzgwHkdl2"},"source":["type(y) # y is also a multi-dimensional array"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AlD0UCdUkjg6"},"source":["y.shape # y contains 200 observations"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YpZuQEKvkn0O"},"source":["y.dtype # Two classes encoded as the 64 bit integers 0 and 1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VnFx9zLTk5VE"},"source":["We can use this random data to build a dataframe object."]},{"cell_type":"code","metadata":{"id":"rZRFVmEukrt-"},"source":["df = DataFrame(\n","    X,\n","    columns=['Feature1', 'Feature2', 'Feature3', 'Feature4', 'Feature5']\n",")\n","\n","df['Target'] = y # Adding target variable\n","\n","df['TargetClass'] = Series(\n","    y,\n","    dtype='category'\n",") # Adding the target variable again, but specifying it to be categorical\n","\n","df[:5] # First 5 rows"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I3wpcKoEn-RD"},"source":["The `describe` method shows the summary statistics for the variables in the dataframe object."]},{"cell_type":"code","metadata":{"id":"Lqd6Pg5IoDmU"},"source":["df.loc[:, df.columns != 'Target'].describe() # Exclude the Target variable from the summary"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FLaPbvSImulL"},"source":["A scatter plot matrix shows us the correlation between each set of feature variables for each of the two classes."]},{"cell_type":"code","metadata":{"id":"i73ptXwqlLev"},"source":["px.scatter_matrix(\n","    df,\n","    dimensions=['Feature1', 'Feature2', 'Feature3', 'Feature4', 'Feature5'],\n","    color='TargetClass', # The categorical version of the target variable\n","    title='Scatter plot matrix'\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nG5u9OP8nFTK"},"source":["To use the $k$ nearest neighbour classifier, we instantiate it and specify a value for $k$. We choose $k=5$. There are more arguments available for this classifier, but we leave these at their default values."]},{"cell_type":"code","metadata":{"id":"ZF4T5r9Bl4HH"},"source":["neigh = KNeighborsClassifier(\n","    n_neighbors=5\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TqTEWUBtngAl"},"source":["Next up we fit the data to the instantiated classifier, using the `fit` method."]},{"cell_type":"code","metadata":{"id":"K-l2KqX3nTjC"},"source":["neigh.fit(\n","    X, # The multi-dimensional numpy array of feature variable valuess\n","    y # The numpy array of target variable values\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v7mFC-d4oixw"},"source":["The `predict` method allows us to pass values for an unknown observation and see which class the fitted classifier predicts."]},{"cell_type":"code","metadata":{"id":"b4QpH2EYn3kZ"},"source":["np.random.seed(7)\n","unkown_obs = np.random.randn(5).reshape(1, -1) # A single observation with 5 random values\n","\n","neigh.predict(unkown_obs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6h9X3boypTCH"},"source":["We see a predicted target class of `1`. The `predict_proba` method will return the probability for each target class given an observation."]},{"cell_type":"code","metadata":{"id":"Il4uYaFRo-DM"},"source":["neigh.predict_proba(unkown_obs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3sYD3oLqp442"},"source":["Class `1` was predicted with a $100$% probability."]},{"cell_type":"markdown","metadata":{"id":"9yWjHbS1qPKH"},"source":["### DATA SPLITTING"]},{"cell_type":"markdown","metadata":{"id":"6tR-Am5xqQ5C"},"source":["While we have _trained_ a classifier, we are not sure how well it does. In most ML applications, we randomly split the dataset into a __training set__ and a __test set__. The model trains of the former (as we did above). The latter is not used in the training, but is kept for obtaining metrics on our model."]},{"cell_type":"markdown","metadata":{"id":"DBNBcYJZqpeL"},"source":["This approach allows us to gauge how well a ML model might do on unseen data. This is of obvious importance, as we want to use our model on new data and have it perform well."]},{"cell_type":"markdown","metadata":{"id":"X3hG8cfOq5oY"},"source":["This brings with it the concepts of variance and bias, pertaining to how well the data does on the trainig set and how well it performs on unseen data."]},{"cell_type":"markdown","metadata":{"id":"RWtiV9qXrB-Q"},"source":["High __variance__ refers to a model that does very well on a training set. Such a model __overfits__ the training data and might very well do poorly on unseen data. A model with high __bias__ does rather worse on the training data. To some extent there is a trade-off between these."]},{"cell_type":"markdown","metadata":{"id":"Z4d97JkdrYao"},"source":["A variety of factors influences variance and bias. The sample size is key. The more training data we have in ML, the better the model usually performs. Aspects such as the value of $k$ in our example is termed a __hyperparameter__ of the model. It is _something_ about the model that we choose and must set. Note that there are techniques that can be used to let our computer _search for_ the best hyperparameter values."]},{"cell_type":"markdown","metadata":{"id":"HcAil-9Hr8qu"},"source":["Below, we split the data into a $80$% training set and a remainder of $20$% test set. There is a trade-off here too. More data in the test set gives us a better indication of how well it will do on unseen data. We do then, however, take away observations that could have been used for training."]},{"cell_type":"markdown","metadata":{"id":"SxIaav0cspkR"},"source":["The `train_test_split` function takes various arguments. Below, we set the required arguments. This includes `test_size` set as a fraction of all the observations. We use the commonly used computer variables for the split data. The names are rather explanatory."]},{"cell_type":"code","metadata":{"id":"dk4Z9BCUpf7n"},"source":["X_train, X_test, y_train, y_test = train_test_split(\n","    X,\n","    y,\n","    test_size=0.2,\n","    random_state=42\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mUR1r76Ns7_r"},"source":["It is important to know that we have a fair representation of the classes in both sets. If not, we have __unbalanced__ sets. This is a particulary interesting problem requiring its own solutions. The numpy `unique` function returns the sample space elements in an array. With the `return_counts` argument set to `True`, we also get a frequency count of each class."]},{"cell_type":"code","metadata":{"id":"C6lRLT0bsoZ3"},"source":["np.unique(\n","    y_train,\n","    return_counts=True\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4IrIH_11tTwc"},"source":["np.unique(\n","    y_test,\n","    return_counts=True\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WFhnOk0Ht517"},"source":["There is a fair representation of each class in both the training and the test sets."]},{"cell_type":"markdown","metadata":{"id":"Km0pHsSSuABS"},"source":["Now, we train the classifier again (still with $k=5$)."]},{"cell_type":"code","metadata":{"id":"Mxm4KN3Jtr1F"},"source":["neigh = KNeighborsClassifier(\n","    n_neighbors=5\n",") # Instantiate with k=5\n","\n","neigh.fit(\n","    X_train,\n","    y_train\n",") # Train on the training data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vloZXa9B3l81"},"source":["### METRICS"]},{"cell_type":"markdown","metadata":{"id":"Zps7NJ7UuzPZ"},"source":["Given our trained model, we can now pass the unseen test set of feature variables to the model. The predicted target classes are assigned to the computer variable `y_pred` below."]},{"cell_type":"code","metadata":{"id":"zpk2DVSTuaCW"},"source":["y_pred = neigh.predict(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CbtaJLqluNpK"},"source":["We can now use the predicted target classes to check on various metrics. One important metric is the accuracy. It returns the fraction of values that were precidicted correctly. We use the `accuracy_score` function from the metrics module of the scikit-learn package. As arguments, we pass the actual test target values, `y_test`, and the predicted classes for each test observation, `y_pred`"]},{"cell_type":"code","metadata":{"id":"PXeSy7APuJ6z"},"source":["metrics.accuracy_score(\n","    y_test,\n","    y_pred\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kb7K5i8bvhtu"},"source":["Our model is $90$% accurate on the unseen data."]},{"cell_type":"markdown","metadata":{"id":"mvIvqspR0owH"},"source":["A __confusion matrix__ expresses the accuracy by showing the correctly and incorrectly predicted instances. The information in a confusion matix is clear to understand when plotted. We do this with the `plot_confusion_matrix` function from the metrics module of the scickit-learn package."]},{"cell_type":"code","metadata":{"id":"-WXspDeBvXMW"},"source":["metrics.plot_confusion_matrix(neigh, X_test, y_test);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UU0uJlUy1KtE"},"source":["We see the true class labels along the left edge and the predicted class labels on the bottom edge. Looking at the plot, $17$ class `0` observations in the test set were correctly predicted by the model as class `0`, with $19$ class `1` observations correctly predicted. Three actual class `1` observations were incorrectly predicted to be class `0` and a single actual class `0` case was incorrectly prected to be class `1`."]},{"cell_type":"markdown","metadata":{"id":"DDI8L1fG2AHp"},"source":["What if we changed the $k$ hyperparameter to be $3$?"]},{"cell_type":"code","metadata":{"id":"pFP8Seuw2AgD"},"source":["neigh = KNeighborsClassifier(\n","    n_neighbors=3\n",") # Instantiate with k=3\n","neigh.fit(\n","    X_train,\n","    y_train\n",") # Train on the training data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k2L18AMu257r"},"source":["The confusion matrix plot shows that we did a bit better."]},{"cell_type":"code","metadata":{"id":"5Nilr6Oh2ikC"},"source":["metrics.plot_confusion_matrix(neigh, X_test, y_test);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZS5EsTmR3ELP"},"source":["The accuracy is now up to $92.5$%."]},{"cell_type":"code","metadata":{"id":"07BUnDDE2zwt"},"source":["y_pred = neigh.predict(X_test)\n","\n","metrics.accuracy_score(\n","    y_test,\n","    y_pred\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7N3KU7VhjgTB"},"source":["## $k$ NEAREST NEIGHBOURS CLASSIFIER DATA SCIENCE EXAMPLE"]},{"cell_type":"markdown","metadata":{"id":"9YXRGMBTlqGj"},"source":["### DATA IMPORT"]},{"cell_type":"markdown","metadata":{"id":"3eoa7XYJj13t"},"source":["In this example we take a data set that can be downloaded from the internet. It contains observations for variables pertaining to the microscopic investigation of cells from breast lumps. Some of the observations are benign (non-canceorus) and some are malignant (cancerous). The spreadsheet file is contained in the `data` subfolder on this Google Drive."]},{"cell_type":"code","metadata":{"id":"98tRdLdWMmSZ"},"source":["#drive.flush_and_unmount()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"joT0P97z3DPv"},"source":["# Connect to Google Drive\n","drive.mount('/gdrive', force_remount=True)\n","\n","# Change directory to the DATA folder\n","%cd '/gdrive/My Drive/DATA SCIENCE/DATA'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5bK1e5W1kTXA"},"source":["# Import the spreadsheet file\n","df = read_csv('breast_cancer.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b7rx0eWgk1EU"},"source":["# First five observations\n","df[:5]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TN2woUgilTTh"},"source":["The `info` method gives us information about the dataframe object and the variable data types. We note that `diagnosis` is an object data type (a categorical variable)."]},{"cell_type":"code","metadata":{"id":"VjrnoWaQlAsO"},"source":["# Information about the DataFrame object\n","df.info()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GSqkHJL-lIDE"},"source":["We can delete the `id` and the `Unnamed: 32` columns as it serves no purpose."]},{"cell_type":"code","metadata":{"id":"PiLiGBEWlIxW"},"source":["df.drop(\n","    ['id', 'Unnamed: 32'],\n","    axis=1,inplace=True\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZQVTxdpklsao"},"source":["### DATA SUMMARY"]},{"cell_type":"markdown","metadata":{"id":"-89q9AnDlyUx"},"source":["There is a known class imbalance in this dataset, with more benign disease than malignant disease. We can visualize and enumerate this."]},{"cell_type":"code","metadata":{"id":"iT5s2I15ioxH"},"source":["px.bar(\n","    df,\n","    x='diagnosis',\n","    title='Frequency of the diagnosis classes',\n","    labels={\n","        'diagnosis':'Diagnosis (M for malignant and B for benign)'\n","    }\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9BIStDTGmG0A"},"source":["The fraction of each target class can be calculated using the `value_counts` *method* and setting the `normalize` argument to `True`."]},{"cell_type":"code","metadata":{"id":"UzS3Df4Xl54j"},"source":["df.diagnosis.value_counts(normalize=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wi2AHLkrmKYE"},"source":["The `describe` method is used to give a summary of the rest of the variables. The `loc` indexing is used to exclude the categorical target variable."]},{"cell_type":"code","metadata":{"id":"wrOTCd8QmDzZ"},"source":["np.round(\n","    df.loc[:,df.columns!='diagnosis'].describe(),\n","    1\n",") # Rounding to a single secimal place"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YTirSjyXmRSA"},"source":["All the feature variables are numerical variables (for the kNN classifier). We can generate a correlation matrix to investigate the correlation between all pairs of feature variables, using the `corr` method."]},{"cell_type":"code","metadata":{"id":"ZBo1rksamNFP"},"source":["correlation = df.corr()\n","np.round(correlation, 2) # Rounding to two decimal places"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6u9GvZqLmnWP"},"source":["We can visualize these correlations with a heatmap using the matplotlib package."]},{"cell_type":"code","metadata":{"id":"UWWZ3QJamfDn"},"source":["plt.figure(\n","    figsize=(21, 9)\n",")\n","plt.title('Correlation of features')\n","ax = sns.heatmap(\n","    correlation,\n","    vmin=-1,\n","    vmax=1,\n","    center=0,\n","    cmap=sns.diverging_palette(20, 220, n=200),\n","    annot=True,\n","    fmt='.2f',\n","    linecolor='white'\n",")\n","ax.set_xticklabels(\n","    ax.get_xticklabels(),\n","    rotation=90\n",")           \n","plt.show();"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UfpLZ28pnuOz"},"source":["### DATA SPLITTING"]},{"cell_type":"markdown","metadata":{"id":"dhOHd-QNoA9K"},"source":["Before we split the data into a training and a test set, we need to separate the features variables from the target variable."]},{"cell_type":"code","metadata":{"id":"7nSeEkiYoL37"},"source":["# Generate the computer variable X from df with removal of the diagnosis column\n","y = df.diagnosis # The target variable\n","\n","X = df.drop(\n","    ['diagnosis'],\n","     axis=1\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-YS0MaKbnzte"},"source":["We use the `train_test_split` function again to split $20$% of the data as a test set."]},{"cell_type":"code","metadata":{"id":"7MbTuTXZm3eG"},"source":["X_train, X_test, y_train, y_test = train_test_split(\n","    X,\n","    y,\n","    test_size=0.2,\n","    random_state=12\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FaI_aDYaotHE"},"source":["We review the dimensions of the training and test sets."]},{"cell_type":"code","metadata":{"id":"-WMNkkO1oqLi"},"source":["X_train.shape, X_test.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UUNAZN7XmJfj"},"source":["y_train.shape, y_test.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rfjcLPeupOPf"},"source":["### DATA SCALING"]},{"cell_type":"markdown","metadata":{"id":"LGSPMdMopQec"},"source":["Scaling data is an important step in ML. It puts all the variables within a similar numerical interval. This has advantages for the training step of many ML algorithms. There are various ways to scale data. Here, we will use __standard scaling__, where the mean of a variable is subtracted from each value in that variable and this difference individed by the standard deviation of that variable, shown in (3), where $z$ is the scaled value, $x_{i}$ is each value for the variable, $\\bar{x}$ is the mean and $s_{X}$ the standard deviation of the variable."]},{"cell_type":"markdown","metadata":{"id":"h3O0tQQkqkmu"},"source":["$$z = \\frac{x_{i} - \\bar{X}}{s_{X}} \\tag{3}$$"]},{"cell_type":"markdown","metadata":{"id":"Fuf3U4VUp6VH"},"source":["To use this scaler, we instantiate the class."]},{"cell_type":"code","metadata":{"id":"lXu7E4pipE1t"},"source":["# Generating an instance of the StandardScaler class with default argument values\n","scaler = StandardScaler(\n","    copy=True,\n","    with_mean=True,\n","    with_std=True\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wcsuVd5oq7RB"},"source":["The training set is first fitted and then transformed (in one step) to the scaler using the `fit_transform` method."]},{"cell_type":"code","metadata":{"id":"vpvTtDDgqyrR"},"source":["X_train = scaler.fit_transform(X_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UlVAS4IVrTUf"},"source":["The test data is transformed with the attributes of the scaling of the training set. This is very important. The test set must not be scaled using its own mean and standard deviation."]},{"cell_type":"code","metadata":{"id":"k-GA5JX0rKrK"},"source":["X_test = scaler.transform(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4MdmWnItsJ9n"},"source":["### TRAINING"]},{"cell_type":"markdown","metadata":{"id":"j3b0gyNjsPuf"},"source":["We follow the same steps as used in our initial introduction to the kNN classifier. We will use $k=3$ as hyperparameter value."]},{"cell_type":"code","metadata":{"id":"7zfkx2Nyrhwg"},"source":["# Instantiating the classifier with k=3\n","knn = KNeighborsClassifier(\n","    n_neighbors=3\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I8xotM7zsjf7"},"source":["# Fit the training set\n","knn.fit(\n","    X_train,\n","    y_train\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WbV38TLPtBx2"},"source":["### METRICS"]},{"cell_type":"markdown","metadata":{"id":"BPJ2rxoXtE1N"},"source":["The confusion matrix plot shows how well the model faired when using the unseen test data."]},{"cell_type":"code","metadata":{"id":"_oiEmxpls-rC"},"source":["metrics.plot_confusion_matrix(\n","    knn,\n","    X_test,\n","    y_test\n",");"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XDb72bxCtYtC"},"source":["The accuracy is calculated below using the `accuracy_score` function."]},{"cell_type":"code","metadata":{"id":"CEzGKM8UtTIG"},"source":["metrics.accuracy_score(\n","    y_test,\n","    knn.predict(X_test)\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KzLaJ3kBt9Mt"},"source":["We compare this to the accuracy of the model using the training set, this time using the alternative approach of the `score` method of the model."]},{"cell_type":"code","metadata":{"id":"9KuN0KYQtqo_"},"source":["knn.score(\n","    X_train,\n","    y_train\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C0vqjpJnufMp"},"source":["This gives the same result as the `accuracy_score` function."]},{"cell_type":"code","metadata":{"id":"VjxPtWzqt78V"},"source":["metrics.accuracy_score(\n","    y_train,\n","    knn.predict(X_train)\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y_YQS4olumWx"},"source":["There is definitely some overfitting (high variance) as the model does better on the training data than on the test data."]},{"cell_type":"markdown","metadata":{"id":"xdEXYy--vY_j"},"source":["We can also look at the balanced accuracy score using the `balanced_accuracy_score` function. This metric allows for class imbalance."]},{"cell_type":"code","metadata":{"id":"dDS1AgiSuNtQ"},"source":["metrics.balanced_accuracy_score(\n","    y_test,\n","    knn.predict(X_test)\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y8cy12ZvvobB"},"source":["This score is still much better than the __null score__, which is the fraction of the majority class. Since `y_test` is a pandas series object, we can use the `value_counts` method with the `normalize` argument set to `True`."]},{"cell_type":"code","metadata":{"id":"b98o-CIRvVED"},"source":["# Null or baseline score based on proportion majority class\n","y_test.value_counts(\n","    normalize=True\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6JU2GcIpwSlr"},"source":["The majority class in the test set is `B` (benign disease), with a fraction of $0.58$. If we simply use the majority class as predictor, we would be correct $58$% of the time. Our model therefor improves our prediction."]},{"cell_type":"markdown","metadata":{"id":"WYHmoJdr4qAk"},"source":["There are other more important metrics such as the sensitivity (recall), the specificity, the positive predictive value (precision), and the negative predictive value. All of these required us to understand the concepts of true and false positive and negatives."]},{"cell_type":"markdown","metadata":{"id":"UzoCJokQ5F97"},"source":["The two classes in our target variable for the current example is `B` for benign and `M` for malignant. As data scientists, we choose one of these classes as our _class of interest_, i.e. the one that we want to predict. In this case, it can be `M`. Given an unknown new observation (set of values for all the feature variables), the model will predict `M` with some probability. We can set a threshold on the interval $\\left[ 0,1 \\right]$. If the probability for `M` is above the threshold, the model predicts `M`, else it predicts `B`. We can set this threshold depending on how costly mistakes (in either direction) are given the cicumstances in which the model is used. By default, the threshold is $0.5$."]},{"cell_type":"markdown","metadata":{"id":"VDmdhbyy6d0L"},"source":["If we look at the first observation in the test set, we note that its true class was `M`."]},{"cell_type":"code","metadata":{"id":"yd35ertc5yAt"},"source":["y_test.iloc[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6kmV7v_d6mBD"},"source":["We can use the first row in the feature set to see what the model predicts."]},{"cell_type":"code","metadata":{"id":"1gf9Z-E451iM"},"source":["knn.predict(X_test[0].reshape(1, -1))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BSG1bgaD6s-o"},"source":["So, if our class of interest was `M`, then the model's prediction would be termed a __true positive__. Here positive refers to the class of interest. If the predicted class was `B`, this would be a __false negative__. Here negative is assigned purely on our research approach and which class we are interested in."]},{"cell_type":"markdown","metadata":{"id":"557b5mri6_E_"},"source":["If the true class was `B` and the model predicted a `B`, then this would be a __true negative__. If it predicted `M`, though, it would a `false positive`."]},{"cell_type":"markdown","metadata":{"id":"wTxTDSJt7TP4"},"source":["The following abbreviations are often used. The values from our last confusion matrix plot are added under the assumption of `M` being our positive class.\n","\n","| Metric           | Abbreviation    | Example value |\n","| :----------------|:----------------|:--------------|\n","| True positive    | TP              | 42            |\n","| True negative    | TN              | 66            |\n","| False positive   | FP              | 0             |\n","| False negative   | FN              | 6             |\n"]},{"cell_type":"markdown","metadata":{"id":"Qi_g7ovL86Vz"},"source":["The equations for our four new metrics are shown in (4), where PPV is positive predictive value and NPV is negative predictive value."]},{"cell_type":"markdown","metadata":{"id":"IQNrBoQg89hd"},"source":["$$ \\begin{align} \\text{sentitivity (recall)} &= \\frac{TP}{TP + FN} \\\\ \\text{specificity} &= \\frac{TN}{TN + FP} \\\\ \\text{PPV (precision)} &= \\frac{TP}{TP + FP} \\\\ \\text{NPV} &= \\frac{TN}{TN + FN} \\end{align} \\tag{4}$$"]},{"cell_type":"markdown","metadata":{"id":"D4V4Cu6T952r"},"source":["__Recall__ is then the fraction of true positive cases predicted as such by the model. __Specificity__ is the fraction of true negative cases predicted as such by the model. __Precision__ is the fraction of cases that were correctly predicted as positive over all the cases that were predicted as positive. __Negative predictive value__ is the fraction of cases that were truely negative over all the cases that were predicted to be negative."]},{"cell_type":"markdown","metadata":{"id":"xaPIBYJy-qx-"},"source":[">These metrics are domain specific. In healthcare for instance, the sensitivity (more often used in this setting than the Data Science term recall) is the ability of a test to return a positive result given all truely positive cases. Sensitivity is the ability of a test to correctly identity truely negative cases. Positive predictive value (more often used in this setting than the Data Science term precision) is used after the test returns a positive result and expresses the probability of the actual result being positive. Finally, negative predictive value is also used after a test is done and expresses the probability of a negative result actually being negative."]},{"cell_type":"markdown","metadata":{"id":"oX0g0AcR_hM5"},"source":["Another metric is the __f1 score__. This reflects the _balance_ between the precision and recall, as shown in (5)."]},{"cell_type":"markdown","metadata":{"id":"Lp03lPOoAIbk"},"source":["$$f_{1} = 2 \\times \\frac{\\text{precision} \\times \\text{recall}}{\\text{precision} + \\text{recall}} = \\frac{\\text{TP}}{TP + \\frac{1}{2} \\left(FP + FN \\right)} \\tag{5}$$"]},{"cell_type":"markdown","metadata":{"id":"vP9989G2AA2Z"},"source":["Some of the metrics are returned using the `classification_report` function."]},{"cell_type":"code","metadata":{"id":"ewmTQ9pu55uG"},"source":["print(metrics.classification_report(\n","    y_test,\n","    knn.predict(X_test)\n","))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mMiowWdqA3d-"},"source":["We have raised the question as to when a value is predicted as a certain class. The kNN algorithm produces a probability for each class (the fraction of actual $k$ classes in the neighbourhood of an observation. Since we are dealing with an odd number of neigbours, the majority class in this neigbourhood _rules_. Below, we look at the first $10$ probabilities predicted from the test set feature variables."]},{"cell_type":"code","metadata":{"id":"1hlO8Z3R_vHF"},"source":["knn.predict_proba(X_test)[0:10]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"moK23iWBBYzT"},"source":["For most cases all three nearest neighbours were of the same class, but in three of them only two of the neighbours were of the same class."]},{"cell_type":"markdown","metadata":{"id":"J9gOfKTaBtwd"},"source":["Given larger values of $k$ we will see different probabilities. Below, we choose $k=7$ and look at the accuracy metric and at the first $10$ observation probabilities again."]},{"cell_type":"code","metadata":{"id":"ycAW582ZB9mo"},"source":["# Instantiate the classifier\n","knn_7 = KNeighborsClassifier(\n","    n_neighbors=7\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qB8QYxRGA06K"},"source":["# Train the model\n","knn_7.fit(\n","    X_train,\n","    y_train\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mxLEiDPPCGqX"},"source":["# Accuracy on the test set\n","knn_7.score(\n","    X_test,\n","    y_test\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Dp2V1ecaCae7"},"source":["We get an improved accuracy."]},{"cell_type":"code","metadata":{"id":"qnTKsEPYCVeU"},"source":["knn_7.predict_proba(X_test)[0:10]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1H8jDsttDPwa"},"source":["Below, we generate a DataFrame object from these probabilities (using the test set)."]},{"cell_type":"code","metadata":{"id":"2CuJ5INiCgiI"},"source":["probabilities = DataFrame(\n","    knn_7.predict_proba(X_test),\n","    columns=['B', 'M']\n",")\n","\n","probabilities[:10]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SLOo4CP7DYWE"},"source":["We can now create a bar chart to show the frequency of each probability for the `M` class."]},{"cell_type":"code","metadata":{"id":"EzI4m-RHC0LY"},"source":["px.bar(\n","    np.round(probabilities, 2),\n","    x='M',\n","    title='Frequency of probabilities for malignant class',\n","    labels={'M':'Probabilities of M'}\n",").update_xaxes(\n","    type='category'\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VY_czhpMDicG"},"source":["All the observations that had a probability of `M` greater than $50$% ($0.5$) is predicted to be `M` by the model. What if we change this threshold, though? This can be done depending on how _expensive_ mistakes are. If it is costly to miss a positive results, then we can set the threshold lower so that more observations are predicted to be positive. The meaning of the term _expensive_ is determined by the setting of the Data Science project."]},{"cell_type":"markdown","metadata":{"id":"ZxHkdC_iEMTz"},"source":["A __receiver operator characteristic__ (ROC) curve presents a visual representation of different thresholds. The _x_ axis of this plot is $1 - \\text{specificity}$ and the _y_ axis is the recall. To use this plot, we first calculate the required values using the `roc_curve` function."]},{"cell_type":"code","metadata":{"id":"3927QCupFZ3Y"},"source":["fpr, tpr, thresholds = metrics.roc_curve(\n","    y_test.replace(['B', 'M'], [0, 1]),\n","    knn_7.predict_proba(\n","        X_test\n","    )[:, 1]\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0SsnA_XhKkLV"},"source":["Below, we use the matplotlib package to generate the curve."]},{"cell_type":"code","metadata":{"id":"9LgFlHZfC-YR"},"source":["plt.plot(fpr, tpr, linewidth=2)\n","plt.plot([0,1], [0,1], 'o--')\n","plt.rcParams['font.size'] = 12\n","plt.title('ROC curve kNN classifier')\n","plt.xlabel('False Positive Rate (1 - Specificity)')\n","plt.ylabel('True Positive Rate (Sensitivity)')\n","\n","plt.show();"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dnKJSvXOMn0c"},"source":["The orange dotted line represents a $50:50$ _chance_. We want our curve to be higher than this line, which indeed it is. For a specificity of just under $100$% (to the left of the _x_ axis), we get almost $90$% recall."]},{"cell_type":"markdown","metadata":{"id":"9ydBzjpIK8sm"},"source":["The area under the curve(ROC AUC) represents the ROC score. The closer the ROC AUC is to $1.0$ the better the model performance. The `roc_auc_score` function calucates this area. Note that we use the numpy `where` function to replace `B` with $0$ and `M` with $1$ since we need numerical values."]},{"cell_type":"code","metadata":{"id":"ZLqaqgFRLOvb"},"source":["metrics.roc_auc_score(\n","    y_test.replace(['B', 'M'], [0, 1]),\n","    np.where(\n","        knn_7.predict(X_test) == 'B', 0, 1\n","    )\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dLopYavdL_q4"},"source":["The ROC score or ROC AUC is $0.95$, which is very good."]},{"cell_type":"markdown","metadata":{"id":"sGDYpduSNRl3"},"source":["For both kNN classifiers ($k=3$ and $k=7$) we have only performed a single training step. We might have been very _lucky_ or _unlucky_ in the random split of a training and a test set. It is better to repeat this process many times over. This is termed cross-validation."]},{"cell_type":"markdown","metadata":{"id":"ij6hwiNpNoSk"},"source":["### CROSS VALIDATION"]},{"cell_type":"markdown","metadata":{"id":"tmPDPtXbNnEj"},"source":["With __cross-validation__ we split the data repeatedly and measure performance metrics, over which we average in the end. In $k$ fold cross validation, we choose a number of folds. As an example, we might choose $k=5$. Note that this is not the $k$ of kNN. For a $5$ fold cross validation, the data is split into training and test sets five times, such that all the data is used for training and testing."]},{"cell_type":"markdown","metadata":{"id":"WSTJ2W-5OcJJ"},"source":["When the `scoring` argument is set to `accuracy`, the `cross_val_score` function from the model_selection module of the scikit-learn package returns the accuracy $k$ number of times."]},{"cell_type":"code","metadata":{"id":"u2R-souoFGTo"},"source":["scores = cross_val_score(\n","    knn_7,\n","    X,\n","    y,\n","    cv=5,\n","    scoring='accuracy'\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FHzKQNgCPEWV"},"source":["scores"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n8fEAv-pPTWv"},"source":["The average of these scores gives a better understanding of model performance on unseen data."]},{"cell_type":"code","metadata":{"id":"3iHZk4x4PFfP"},"source":["np.mean(scores)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4VGVb_K4Pfvl"},"source":["np.min(scores), np.max(scores)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ikkyRQftPwDG"},"source":["There is quite a large range for these scores, indicating that the accuracy is very dependent on which observations are in the training and the test sets. In this case, it is a function of the small number of observations in the data set."]},{"cell_type":"markdown","metadata":{"id":"dHS_janhQDWM"},"source":["The final question in this section is which value of $k$ for the kNN classifier is best. One method for finding the optimal value is to perform a grid search."]},{"cell_type":"markdown","metadata":{"id":"6YkYiOX6QfQ-"},"source":["### GRID SEARCH FOR BEST HYPERPARAMETER VALUES"]},{"cell_type":"markdown","metadata":{"id":"Cs3aMKNTQiiA"},"source":["With a grid search we can explore the solution space for hyperparameter values. This process is known as __hyperparameter tuning__."]},{"cell_type":"markdown","metadata":{"id":"QHGSVOzyQxf8"},"source":["The hyperparameters we will tune for the kNN classifier here are the leaf count, the number of neighbours, and the distance metric. While we used Euclidean distance in this notebook, there are other distance metrics too. This argument is set using the `p` value when instatiating the classifier. The leaf count pertains to the search algorithm used for determining the closest neighbours. The kNN classifier used in scikit-learn can use a few of these algorithms such as the KD-tree algorithm or the ball-tree algorithm, or even a brute force approach."]},{"cell_type":"markdown","metadata":{"id":"moBJ3WKRRT6_"},"source":["To use a grid search, we set values to explore."]},{"cell_type":"code","metadata":{"id":"WT9brpsQPpmM"},"source":["# Generate a Python list of leaf size values on the closed interval 1 through 49\n","leaf_size = list(range(1, 50))\n","\n","# Generate a Python list of neigbour numbers on the closed interval 1 through 19\n","n_neighbors = list(range(1, 20))\n","\n","p = [1, 2]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VOncHUHgR_4k"},"source":["We convert these lists to a dictionary."]},{"cell_type":"code","metadata":{"id":"iZNwIWyPSF6v"},"source":["hyperparams = {\n","    'leaf_size':leaf_size,\n","    'n_neighbors':n_neighbors,\n","    'p':p\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VkXRObQbR4qD"},"source":["Next we instantiate a new kNN classifier with default argument values."]},{"cell_type":"code","metadata":{"id":"_GuhPrKrRwll"},"source":["knn = KNeighborsClassifier()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zFjRJZmlS9EI"},"source":["Now we perform the grid search, which will go through all the hyperparameter values when we fit the data. We also use $5$ fold cross validation. All of this can be computationally expensive (consuming a lot a computer resources and taking a long time). "]},{"cell_type":"code","metadata":{"id":"qzlZ7AVKR-Rp"},"source":["knn_grid_search = GridSearchCV(\n","    knn,\n","    hyperparams,\n","    cv=5\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2kc2X46ZTUTT"},"source":["best_params = knn_grid_search.fit(\n","    X,\n","    y\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s1AFBxFuT_Pc"},"source":["The process took over 90 seconds on Colab."]},{"cell_type":"markdown","metadata":{"id":"gxtSkfWbUGtH"},"source":["Now we can print the best hyperparameter values using the `best_estimator.get_params` method."]},{"cell_type":"code","metadata":{"id":"rgHOyVO9UNVe"},"source":["# Best leaf size\n","best_params.best_estimator_.get_params()['leaf_size']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"350uORBTUWgt"},"source":["# Best number of neighbours\n","best_params.best_estimator_.get_params()['n_neighbors']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iAo5_y3CUcqd"},"source":["# Best distance metric\n","best_params.best_estimator_.get_params()['p']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sRxo_mFqUwTD"},"source":["Below, we use these hyperparameter values and $5$ fold cross validation. Note that these may be different every time you run the code. Below, we see the best parameter values generated during a previous run."]},{"cell_type":"code","metadata":{"id":"h_tVJvLlU2gq"},"source":["knn_best = KNeighborsClassifier(\n","    n_neighbors=9,\n","    leaf_size=9,\n","    p=1\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"43TnhurMU_A_"},"source":["scores = cross_val_score(\n","    knn_best,\n","    X,\n","    y,\n","    cv=5,\n","    scoring='accuracy'\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O2ej878nVK9q"},"source":["The average accuracy is now better than before."]},{"cell_type":"code","metadata":{"id":"wUlzemTEVFYx"},"source":["np.mean(scores)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GPZJk0a1W3Rp"},"source":["## $k$ NEAREST NEIGHBOUR REGRESSION"]},{"cell_type":"markdown","metadata":{"id":"Iapv4T8GXAOT"},"source":["The $k$ nearest neighbour (kNN) algorithm can also be used for regression. Here the target variable is a continuous numerical variable."]},{"cell_type":"markdown","metadata":{"id":"Q6tYTqh5XGKE"},"source":["### GENERATING A DATA SET"]},{"cell_type":"markdown","metadata":{"id":"vNVWAXnYXKQ8"},"source":["To understand the basic concept of building a kNN regression model, we start by generating a data set, with a single feaure variable, and then visualise the data."]},{"cell_type":"code","metadata":{"id":"6a_khi3sXKze"},"source":["X = np.arange(\n","    start=1,\n","    stop=11\n",")\n","\n","X # A 10 element array"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ICSYNuShXOHY"},"source":["y = 2 * X\n","\n","y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QnKn6XFRXUsu"},"source":["go.Figure(\n","    go.Scatter(\n","        x=X,\n","        y=y,\n","        name='Data',\n","        mode='markers',\n","        marker={\n","            'size':20\n","        }\n","    )\n",").update_yaxes(\n","    scaleanchor='x',\n","    scaleratio=1\n",").update_layout(\n","    title='Regression data',\n","    xaxis={'title':'Feature variable'},\n","    yaxis={'title':'Target variable'}\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j2FCtKqUXX_H"},"source":["### CREATING A MODEL"]},{"cell_type":"markdown","metadata":{"id":"vqYpDiqzXb1j"},"source":["We instantiate a kNN regressor with $k=3$ nearest neighbours. The leaf size, search method, and distance measure arguments are left at their default values."]},{"cell_type":"code","metadata":{"id":"1dfVogj4XXhq"},"source":["# Instantiating a kNN regressor with k=3 neighbours\n","knn = KNeighborsRegressor(\n","    n_neighbors=3\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Sh44U3fpXhkq"},"source":["### TRAINING THE MODEL"]},{"cell_type":"markdown","metadata":{"id":"r9NPHoOiXkRr"},"source":["Next, we fit the data to the model for training."]},{"cell_type":"code","metadata":{"id":"b8OWwW-JXiHJ"},"source":["knn.fit(\n","    X.reshape(-1, 1),\n","    y\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wslfuP7fXo2Q"},"source":["### TESTING THE MODEL"]},{"cell_type":"markdown","metadata":{"id":"9byaG6eTXueu"},"source":["We can now pass a value to the model to see what it predicts and then try to understand the result."]},{"cell_type":"code","metadata":{"id":"x7orSUiYX3gO"},"source":["knn.predict(np.array([5.5]).reshape(1, -1))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-YX9YlVbXxQE"},"source":["We see a result of $10$. We can plot this to visualize the prediction in view of the data."]},{"cell_type":"code","metadata":{"id":"UoEmUOOQX0Mu"},"source":["go.Figure(\n","    go.Scatter(\n","        x=X,\n","        y=y,\n","        name='Data',\n","        mode='markers',\n","        marker={\n","            'size':20\n","        }\n","    )\n",").add_trace(\n","    go.Scatter(\n","        x=[5.5],\n","        y=[10],\n","        name='Unseen data',\n","        mode='markers',\n","        marker={\n","            'size':20\n","        }\n","    )\n",").update_yaxes(\n","    scaleanchor='x',\n","    scaleratio=1\n",").update_layout(\n","    title='Regression data',\n","    xaxis={'title':'Feature variable'},\n","    yaxis={'title':'Target variable'}\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G8pjuXrUX6jg"},"source":["The three nearest observations have target variable values of $9, 10$, and $11$. The average of this is $10$."]},{"cell_type":"markdown","metadata":{"id":"a5RZN0pJRE-y"},"source":["## CONCLUSION"]},{"cell_type":"markdown","metadata":{"id":"EbXRzhBdRJ8w"},"source":["This notebook was an introduction to the world of machine learning using one of the most interpretable algorithms. Through the examples, we have gained valueble knowledge of terms used in machine learning, the construction of these models, and how to evaluate them."]},{"cell_type":"code","metadata":{"id":"8ma-_9D0X7Jz"},"source":[""],"execution_count":null,"outputs":[]}]}