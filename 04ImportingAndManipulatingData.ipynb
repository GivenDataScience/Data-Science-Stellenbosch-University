{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"04ImportingAndManipulatingData.ipynb","provenance":[{"file_id":"15d1_OJ5LgrGmQWRJi7kqLSxIFgqJPldF","timestamp":1619954375182},{"file_id":"1Ei9fshaRX17n0GAX9Fn7qFp1oZbrYw5t","timestamp":1585032678385},{"file_id":"14H-2iUPAy2yZ61ctN90p3I2GxKvGBCMF","timestamp":1584724590567},{"file_id":"1IJfBx6NiPmSishBWFoR8z8Bk1RveDvXC","timestamp":1583753336690}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"qilfZ3S2hMiw"},"source":["# IMPORTING AND MANIPULATING TABULAR DATA"]},{"cell_type":"markdown","metadata":{"id":"it3RhcCthOv6"},"source":["> by Dr Juan H Klopper\n","\n","- Research Fellow\n","- School for Data Science and Computational Thinking\n","- Stellenbosch University"]},{"cell_type":"markdown","metadata":{"id":"pyE4sN0j0Zfv"},"source":["## INTRODUCTION"]},{"cell_type":"markdown","metadata":{"id":"roaWrNKdacMm"},"source":["Data Science by its name and nature requires us to have acces to data. We have learned that images, sounds files, text, and much more pieces of information can be represented as data. In this course, we concentrate on tabular data."]},{"cell_type":"markdown","metadata":{"id":"pwTlTrDca59V"},"source":["Tabular data is data in rows and columns, either extracted from an image, a database, or similar structures and represented in an array. An array is a set of values in rows and columns. As in the case of colour images, it can these rows and column can also be stacked _on top_ of each other. We will consider only sincgle _stacks_ with data in a spreadsheet. There is a fantastic package for importing such tabular data."]},{"cell_type":"markdown","metadata":{"id":"hs7na70XD8yl"},"source":["The **pandas** package has much to do with the success of Python as a programming language for Data Science.  It is an enormous package and is used to import data, to manipulate data, to do calculations with data, and even create graphs and plots using the data."]},{"cell_type":"markdown","metadata":{"id":"wLXl2NfLENTX"},"source":["In this notebook, we are going to get a glimpse into the usefulness of the pandas package by importing some data captured in a spreadsheet file.  We will then extract some of the data that is of interest to us.  In later notebooks, we will do all sorts of useful analysis on the extracted data."]},{"cell_type":"markdown","metadata":{"id":"Ma3TJnB4SF63"},"source":["## PACKAGES FOR THIS NOTEBOOK"]},{"cell_type":"markdown","metadata":{"id":"KVhL-zS602lc"},"source":["It is useful to import all packages at the start of a notebook. This allows us to keep track of what we are using in the notebook."]},{"cell_type":"code","metadata":{"id":"Ij2TePRIhLIz"},"source":["import pandas as pd  # Package to work with data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qUwjfRaZTKLE"},"source":["import numpy as np  # Numerical analysis package"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kcZ7ulTU1AXq"},"source":["To import a file from a Google Drive, we need a special function. This is not required when running Python on a local system, where we can simply refer to the _address_ of the file on the internal (or network) drive."]},{"cell_type":"code","metadata":{"id":"cZiX5j0WSRIt"},"source":["from google.colab import drive  # Connect to Google Drive"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7ahrYPfo1YaC"},"source":["Below, we us a magic command, `%load_ext` to load a `google.colab.data_table`. It produces better tables when using Colab and printing such tables to the screen."]},{"cell_type":"code","metadata":{"id":"V_MvBHHHS0JJ"},"source":["# Format tables printed to the screen (don't put comment on the same line as the code)\n","%load_ext google.colab.data_table"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2dGZxvZnSK9E"},"source":["## IMPORTING DATA"]},{"cell_type":"markdown","metadata":{"id":"ldphY4KW1rsK"},"source":["In Google Colaboratory, we have to _mount_ the cloud drive with our data file. As mentioned above, this setp is not required when using a local system. When running the cell below a link appears that you have to click on. A new tab will open up in your browser. You have to sign in to your Google account again giving permission to this Colab notebook to read files from your Google Drive (all in the name of security which is important). Once this is done a tab will open with a secuirty link that you have to copy (there is a convenient icon next to the secuirty code that will copy it). Copy it an close the tab. Then you have to paste the security key into the generated box below the code and hit enter or return."]},{"cell_type":"code","metadata":{"id":"BKVNiEhPSJ3I"},"source":["drive.mount('/gdrive', force_remount=True)  # Connect to Google Drive\n","# With force_remount=True we can run this cell again later if needed"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NJNtzNSN2Re8"},"source":["Now we navigate to the desired folder in Google Drive by specifying its address as a string. The address is a string and goes in a set of quotation marks. When you explore your own Google Drive simply note the location of a file if you have data stored somewher else."]},{"cell_type":"markdown","metadata":{"id":"s4G8BvJLeyTY"},"source":["Note that you can also import a data file from your local system. There is a code snippet that you will find under the code snipper icon on the top left of this Colab notebook. The icon is the `<>` icon under the magnifying class for searches. You can scroll down the list to find other useful code snippets that you can use in your projects."]},{"cell_type":"markdown","metadata":{"id":"YRKRdL-rfqxN"},"source":["In the cell below, we see the `%cd` magic command that let's us change directory."]},{"cell_type":"code","metadata":{"id":"PVwZmbhZSTWb"},"source":["%cd '/gdrive/My Drive/DATA SCIENCE/DATA'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Gtpn3I0ogWRf"},"source":["The `%ls` magic command will print a list of the files in the directory to which we changed into."]},{"cell_type":"code","metadata":{"id":"9N3PHNCGgUa9"},"source":["%ls"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0VWEWecRdYuS"},"source":["We note that there is a csv file called `customer_data.csv`.  We can import it using pandas' `read_csv()` function.  Since it is not a Python function, we have to specify where (from what package) it came from.  This is done by preceding the function with the pandas namespace abbreviation that we used initially, `pd`."]},{"cell_type":"code","metadata":{"id":"Y0Q4jLAnSdP9"},"source":["df = pd.read_csv('data.csv')  # Import the spreadsheet file"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x4_QPcBCf_eF"},"source":["Since we navigated to this directory with the `%cd` magic command, we only need to type in the name and extension of your spreadsheet file (using quotation marks as it is a string)."]},{"cell_type":"markdown","metadata":{"id":"kUhztBW6goKV"},"source":["The `type` function used below shows that the object assigned to the `df` computer variable is a DataFrame object."]},{"cell_type":"code","metadata":{"id":"KOxtRdbveu8_"},"source":["type(df)  # Type of the object held in the computer variable df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hZNyLD1BdvoA"},"source":["We can look at the attributes and methods of dataframe objects using Python's `dir` function."]},{"cell_type":"code","metadata":{"id":"xlUT-0aqeE2X"},"source":["dir(df)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lkSv8oQ1eJBc"},"source":["There is quite a lot of them.  Thw first few are the statistical variables (column headers in the first row of the spreadsheet). We can explore each and every one of the rest of the methods and attributes on the Pandas page for [DataFrame objects](https://pandas.pydata.org/pandas-docs/stable/reference/frame.html)."]},{"cell_type":"markdown","metadata":{"id":"rU7kzYmuea9g"},"source":["Once such method is the `head` method.  By default it returns the first five rows of a dataframe object.  An integer value can be passed as argument if we need a different number of rows."]},{"cell_type":"code","metadata":{"id":"Lgr0O4AjSoy-"},"source":["df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NdpI3_eRhcu-"},"source":["Since we used the `%load_ext google.colab.data_table` magic command at the start of the notebook, the dataframe object is printed to the screen in a very neat and useful way, allowing us to navigate the data."]},{"cell_type":"markdown","metadata":{"id":"LkcK7Hc-eysA"},"source":["The `shape` attribute (property) shows use the number of rows and columns, returned as a tuple. Note that unlike a method (which is like a Python function), an attribute has no parentheses."]},{"cell_type":"code","metadata":{"id":"zIGBYrxKS51W"},"source":["df.shape  # Nuber of rows (subjects) and columns (statistical variables)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WRp8zYJdiC2N"},"source":["There are $200$ observations (rows) and $13$ statistical variables (columns) in this _tidy_ data set."]},{"cell_type":"markdown","metadata":{"id":"YYM0QP-Te9Nv"},"source":["The `columns` property list all the column header names, called **labels**."]},{"cell_type":"code","metadata":{"id":"JBDaPKXUS-CR"},"source":["df.columns  # List the statistical variables"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"psVptbI3fU7T"},"source":["The majority of DataFrame objects will have two axes (rows and columns).  We can verify this using the `ndim` attribute."]},{"cell_type":"code","metadata":{"id":"QKjbDaDVfMWl"},"source":["df.ndim"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F4mVHGqVfhn4"},"source":["The `size` attribute gives us the total number of data point values (the product of the number of rows and columns)."]},{"cell_type":"code","metadata":{"id":"dPuz8uybfeaF"},"source":["df.size"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uCWNM3iaftZX"},"source":["The last attribute that we will take a look at is the `dtype` attribute.  It returns the Python data type of the values in each of the columns. This is a very important step. Pandas does its best to interpret the data type. Dependening on how the spreadsheet was created and how dat was entered, it is not always possible to correctly interpret the type. In this case we might have to change the data type. Remember that we base analysis of data on the dat type of the variable."]},{"cell_type":"code","metadata":{"id":"lpmimrU5f4Ih"},"source":["df.dtypes"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EgFqi8ih2w2I"},"source":["Categorical variables are denoted as an `object` type. Numerical variable can be either integer or floating point numbers (numbers with decimal places). These are `int64` and `float64` (denoting 64-bit precision) respectively."]},{"cell_type":"markdown","metadata":{"id":"t9Wmm_WhjTnL"},"source":["We refer to the data about data as meta data. It is important to view the meta data of any data that you import to make sure that the data did indeed import correctly and to start to learn a little bit about the data."]},{"cell_type":"markdown","metadata":{"id":"axG6ITQaV3Lr"},"source":["## EXTRACTING ROWS AND COLUMNS"]},{"cell_type":"markdown","metadata":{"id":"dXcO-rX4jLpJ"},"source":["To analyse data, we want to extract only certain values. This is a very useful skill."]},{"cell_type":"markdown","metadata":{"id":"36An3u3yV54j"},"source":["Pandas refers to a single column in a dataframe object as a **Series** object.  We can also create standalone series objects, but in the context of analysing data, a standalone series object is perhaps not as useful.  Below, we extract just the *Age* column (statistical variable) and save it as a series object.  The notation uses square brackets, with the column name represented as a string."]},{"cell_type":"code","metadata":{"id":"f_MFoexwWa_w"},"source":["age_column = df['Age'] # Note the use of square brackets"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KySKOWmFWhOf"},"source":["Our new object is indeed a series object."]},{"cell_type":"code","metadata":{"id":"FrdHs86FWfEa"},"source":["type(age_column)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TkdLbfk3WoQ2"},"source":["Since we have no *illegal* characters in the column name such as spaces, we can also make use of dot notation.  Below, we overwrite the `age_column` computer variable by reassigning it (using the same name)."]},{"cell_type":"code","metadata":{"id":"cF_iGFJVW1ta"},"source":["age_column = df.Age # A shorter and more convenient way of extracting a column"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5pbFehuXW5lW"},"source":["We can display the first few rows in the series object with the `head` method."]},{"cell_type":"code","metadata":{"id":"ZUT_3waDXC66"},"source":["age_column.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VDg6u7xtXFkP"},"source":["Here we see further evidence that it is not just a Python list or a numpy array, but a series object, by noting the index column."]},{"cell_type":"markdown","metadata":{"id":"ug3tQ2FwXfHt"},"source":["At times it may be more useful to work with a numpy array, rather than a pandas series. To extract the age values as a numpy array, we use the `.to_numpy` method."]},{"cell_type":"code","metadata":{"id":"zDGj3olnS_ki"},"source":["age = df.Age.to_numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8VJC-Za_knYf"},"source":["The object assigned to the `age` computer variable is a numpy array."]},{"cell_type":"code","metadata":{"id":"OGLSfp-LTdQq"},"source":["type(age)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zG4IxgoRXqfY"},"source":["As a numpy array, it has a number of attributes and methods. We use the `dir` function again to print out all the attributes and methods for this Python object type."]},{"cell_type":"code","metadata":{"id":"WijwKpIuThDi"},"source":["dir(age)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tCzTD3WhX1Nn"},"source":["Below, we look at the minimum value and the maxiumum value in the `age` array, and calculate the average of all the values using the `.min`, the `.max`, and the `.mean` methods."]},{"cell_type":"code","metadata":{"id":"gxcF2S3oTklY"},"source":["age.min() # The minimum value in the array"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UvPO7bW_Ttf2"},"source":["age.max() # The maximum value in the array"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RAfr_a6XTvQM"},"source":["age.mean() # The mean of all the values in the array"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HTGvmuCtYYoY"},"source":["We can specify inidividual rows (subjects) by making use of the `.iloc[]` attribute (or property, which is the term used by pandas) for a dataframe object.  The `iloc` property stands for _integer location_, so we must use integers to specify the row and column numbers.  We add an index value in square brackets for the property.  Below, we extract the first row. Remember than Python is `0` indexed, so the first column has an index of $0$."]},{"cell_type":"code","metadata":{"id":"7Yqzu0H4YdDM"},"source":["df.iloc[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fBwVOanMZ1UQ"},"source":["We can specify certain rows by passing a list of integer values."]},{"cell_type":"code","metadata":{"id":"ms1rtyujZ8Zh"},"source":["df.iloc[[2, 3, 5]] # Rows 3, 4, and 6 (remember, we are starting the indexing at 0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a-IldZWgaCF-"},"source":["Slicing is also allowed.  This is done by specifying a range of values. The range object uses colon notation. Below, we use `0:2`. This includes the indices `0`, and `1`. The last index value in NOT included."]},{"cell_type":"code","metadata":{"id":"Y6Ubyib6aIPN"},"source":["df.iloc[0:2]  # The first and second row"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OsY-FxrYaMKW"},"source":["The columns can also be indexed.  Here we use the *row, column* notation.  Below then, we extract the first five rows, but only for the *DOB* and *Age* variables, which are columns 1 and 2."]},{"cell_type":"code","metadata":{"id":"LDIjs10-afce"},"source":["df.iloc[0:5,[1, 2]]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t03WQUStan5V"},"source":["Above, we passed the rows as a range and the two columns as a list."]},{"cell_type":"markdown","metadata":{"id":"vrcmG-TYbf0V"},"source":["The `.loc[]` property can be used in a similar fashion. Here we can specify the column names (as a list or a slice).  If the index values were not integers, but strings, we could also use those names.  Remeber that the row and column names are referred to as **labels**.  Below, we extract the same labels as we did above.  Note, though, that the range includes the sixth row. When extracting rows and column, ALWAYS use the `row, column` notation. Since we want two columns, we pass them as a Python list object (in square brackets) after the comman. Each column name is passed as a string."]},{"cell_type":"code","metadata":{"id":"rqJpqxdlbYVV"},"source":["df.loc[0:5, ['DOB', 'Age']]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A8Y0-nWqcaCN"},"source":["The `.iat` indexing extracts a single *cell* by using its row and column index."]},{"cell_type":"code","metadata":{"id":"4aFId8m8ciSi"},"source":["df.iat[3, 2]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i-fSo8BRcqx8"},"source":["There is also an `at[]` indexing, which does the same. Here we can specify labels, though."]},{"cell_type":"markdown","metadata":{"id":"V7lWEVbugFCt"},"source":["## FILTERING DATA"]},{"cell_type":"markdown","metadata":{"id":"bZz0xlwujOe6"},"source":["Filtering data is one of the most useful things that we can do with data in a  dataframe object.  In this section, we will start to learn how to filter data by extracting numpy array objects based on criteria that we which to investigate or by creating brand new dataframes."]},{"cell_type":"markdown","metadata":{"id":"chNc9Swur1bf"},"source":["In order to do filtering we use conditionals. We have learned about these in the prviosu notebook. For instance, below we ask if $3$ is greater than $4$ and then if $3$ is equal to $3.0$."]},{"cell_type":"code","metadata":{"id":"v3Q2E8cSsERy"},"source":["# A conditional returns a True or False value\n","3 > 4"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0oam2pl4sMEt"},"source":["# The double equal symbols conditional\n","3 == 3.0"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fEpw_T4kxdLR"},"source":["### FINDING UNIQUE VALUES IN A COLUMN"]},{"cell_type":"markdown","metadata":{"id":"HQwfJL4cxg8c"},"source":["Remember that we refer to the **sample space** of a variable as all the possible values that a variable can take.  This is particulary useful when looking at categorical variables.  The `unique` method is used to find all the sample space elements in a column."]},{"cell_type":"code","metadata":{"id":"FOJn6Cdzx4qU"},"source":["df.Smoke.unique() # Data entries encoded as 0, 1, and 2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XVcnveosyWKo"},"source":["We note that there are three elements in the sample space of this column.  This method is great for *surprises* that might be hidden in a dataframe such as one or more strings in a numerical data column.  A common example would be the *Age* column that has one or two strings such as *thirty-two* in it, instead of 32.  Strings in a numerical data column will prevent calculations on that column and such errors in the data must be corrected.  We will learn how to change values using the `replace` method later in this notebook."]},{"cell_type":"markdown","metadata":{"id":"VnN-m5bXjy3-"},"source":["### AGES OF ALL NON-SMOKERS"]},{"cell_type":"markdown","metadata":{"id":"kAsDECJjnEeY"},"source":["The `Smoke` column contain information about the smoking habits of the respondents in the data set. We have seen above that the sample space contains three integers, `0` for not smoking, `1` for smoking, and `2` for previous smoking."]},{"cell_type":"markdown","metadata":{"id":"Pat2Ua0pj0bQ"},"source":["Here, we are interested in creating an array that contains the ages of only the patients who do not smoke in our dataframe.  To do this, we use indexing directly.  A conditional is used to include only *0* patients (`df.Smoke == 0`).  We then reference the column that we are interested in, which is `Age`, followed by the `to_numpy` method."]},{"cell_type":"code","metadata":{"id":"pRzCN9oqTwh8"},"source":["non_smoker_age = df[df.Smoke == 0]['Age'].to_numpy()\n","non_smoker_age # Print the values to the screen"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ESZOWcs1n999"},"source":["When first using this code, it may seem a bit difficult. It does read rather like an English language sentence, though. _Take the dataframe object. Extract the rows in column `Smoke` that are `0`. For all of these rows return the `Age` values as a numpy array_."]},{"cell_type":"markdown","metadata":{"id":"hjQTR_WN54ro"},"source":["As an alternative, we can use the `loc` indexing, passing a _row_ and a _column_ specification as arguemnts. The _row_ interrogates the `Smoke` column and includes only those with a `0` entry. The _column_ is then specified to the the `Age` column."]},{"cell_type":"code","metadata":{"id":"FYLF2glR5HE9"},"source":["df.loc[df.Smoke == 0, 'Age'].to_numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n8cQHqesqar_"},"source":["The different ways to interact with pandas adds to its power and you can find a way to achieve your data analysis goals that best first your way of work."]},{"cell_type":"markdown","metadata":{"id":"sgJ6ZTjFkfiL"},"source":["Since this is now a numpy array object, we can use methods such as the `mean` method to calculate the average age of all the non-smoking participants."]},{"cell_type":"code","metadata":{"id":"sZXrcT62T_yn"},"source":["non_smoker_age.mean()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EncBWPLhrMnX"},"source":["### NON-SMOKER AGES WHERE SURVEY CHOICE IS 3"]},{"cell_type":"markdown","metadata":{"id":"rT7444m3rdB2"},"source":["We now need to filter by two criteria (two columns), `Age` and `Survey`.  The filtering can either refer to **and** or **or**.  In the first, we require all the criteria to be met and in the second, only one of the criteria need be met (return a `True` value)."]},{"cell_type":"markdown","metadata":{"id":"IbF_kKTLrulE"},"source":["The symbol for **and** is `&` and for **or** is `|`. Below, we use `&` since we want both criteria to be met. Each filter is created in a set of parentheses. the code uses the `row, column` notation."]},{"cell_type":"code","metadata":{"id":"LswrNSudr8AK"},"source":["non_smoker_satisfied_age = df.loc[(df.Smoke == 0) & (df.Survey == 3), 'Age'].to_numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0x_FljqjqB4-"},"source":["In English the code reads: _Take the `df` dataframe object and look down the rows of the `Smoke` and `Survey` columns. Return only the rows where `Smoke` is `0` AND `Survey` is `3`. Then return the `Age` column for all these rows fulfilling both criteria_."]},{"cell_type":"markdown","metadata":{"id":"BzjJpgfEs7sW"},"source":["### NEVER SMOKED OR SATISFACTION SCORE GREATER THAN 3"]},{"cell_type":"markdown","metadata":{"id":"6mWhPxb7tGaJ"},"source":["We are interested in those participants who never smoked OR those that have a satisfaction score of more than `3`. Here our filtering criteria requires only _one_ of the two criteria to return `True`.  A clearer way to build these filtering criteria, is to save them as a computer variable first."]},{"cell_type":"code","metadata":{"id":"qRIr-P_Ltk4O"},"source":["# Saving the filtering criteria as a computer variable\n","# The > symbol is used to indicate greater that 3\n","crit = (df.Smoke == 0) | (df.Survey > 3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1mqqW4-0txjF"},"source":["We can now pass this to the `loc` property (as row and then specify the column name)."]},{"cell_type":"code","metadata":{"id":"-MbLv8MctY61"},"source":["non_smoker_or_satisifed_age = df.loc[crit, 'Age'].to_numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oExrrkoBt9IX"},"source":["### NON-SMOKERS AND SATISFACTION SCORE OF 3 OR LESS"]},{"cell_type":"markdown","metadata":{"id":"Sj_n44CeuFVy"},"source":["Non-smokers are either those who have never smoked (`0`) or those that are ex-smokers (`2`). Both need inclusion. In other words, we need to exclude the smokers. It is easier to select just one element. One way to deal with the interrogation of the data is through negation.  We can change our language into an opposte view, i.e start with filtering the current rows with a score of greater than 3. Then we simply use negation with the tilde, `~`, symbol to exclude these cases."]},{"cell_type":"code","metadata":{"id":"ffPcnZLMuf5L"},"source":["# Include those who do not smoke and have a score of more than 3\n","crit = (df.Smoke == 1) & (df.Survey > 3)\n","\n","# Now we exclude these rows with a ~ negation symbol\n","not_no_smoker_satisfied_age = df.loc[~crit, 'Age'].to_numpy()\n","not_no_smoker_satisfied_age"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HGEzVDslmNQr"},"source":["### CREATE A NEW DATAFRAME OBJECT THAT ONLY CONTAINS PARTICIPANTS YOUNGER THAN 50"]},{"cell_type":"markdown","metadata":{"id":"3JdZhfZymX0w"},"source":["Instead of just an array of values, we want to create a new dataframe object.  (Because it is a part of an existing dataframe object, some Data Scientist refer to it as a sub-dataframe object.)  It includes all the columns (variables), but only for patients up to and including 49 years of age.  This is very simple to achieve."]},{"cell_type":"code","metadata":{"id":"MAra55fFUFd6"},"source":["new_df = df[df.Age < 50]\n","new_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"myk1Am1Fm5HC"},"source":["Let's verify the result of our code by looking at the maximum *Age* value of this new dataframe.  Below, we see three ways to return the maximum value in the new *Age* column."]},{"cell_type":"code","metadata":{"id":"wqKZOckzm-e3"},"source":["new_df.Age.max()  # Using the column name directly"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Es0vEw92nEJz"},"source":["new_df['Age'].max()  # Using the column name as a column index name"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xi2m1RmLnIxV"},"source":["new_df.loc[:, 'Age'].max()  # Using the loc property"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a8opqfJ6nfvx"},"source":["Above we see the shorthand notation for including *all* elements, the colon, `:`.  Since this is the `.loc[]` property, we expect row and column labels.  For the rows then, we use the colon symbol to indicate that we are interested in all the rows.  After the comma we indicate the column label and outside of the `.loc[]` indexing, we use the `.max()` method."]},{"cell_type":"markdown","metadata":{"id":"Wb-JTGYHwe7i"},"source":["### CREATE A NEW DATAFRAME FOR PARTICIPANTS WITH A RESTRICTED LIST OF JOB TITLES"]},{"cell_type":"markdown","metadata":{"id":"WCbKD9WPwxT5"},"source":["Up until now, we have had single values for our filter criteria, even though we had multiple criteria.  Here we might want to filter on more than one value, say *IT consultant*, *Energy manager*, and *Clinical embryologist*. Since the sample space is quite large, negation would not be a good solution as we would need to list all the other `Vocation` sample space values. Here's how we would create the new dataframe object, by making use of the `isin` method."]},{"cell_type":"markdown","metadata":{"id":"ZdAZJERv9sqi"},"source":["We create a list of the sample space elements that we are interested in. We then build a criterium using the `isin` method. Its job is exactely what it sounds like, _is in_, i.e. select only an element that _is in_ the list."]},{"cell_type":"code","metadata":{"id":"AJi57JNvxJQb"},"source":["# Create a Python list object with all the column names\n","jobs = ['IT consultant', 'Energy manager', 'Clinical embryologist']\n","\n","# Build a criterium\n","crit = df.Vocation.isin(jobs)\n","\n","# Create the new dataframe object and print the first 5 rows to the screen\n","jobs_df = df.loc[crit]\n","jobs_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cxriYfwUyVek"},"source":["### CREATE A NEW DATAFRAME WHERE THE WORD _MANAGER_ APPEARS IN THE `VOCATION` COLUMN"]},{"cell_type":"markdown","metadata":{"id":"FMXTVLZVyhOC"},"source":["This filter uses a string method, `str.contains`.  It is ideal for free-form input cells in a spreadsheet, where we can search for keywords.  Below, we see an extra `na=False` argument.  This is used to deal with dataframe obejcts with missing data.  We will learn how to deal with missing data later."]},{"cell_type":"code","metadata":{"id":"9ImKcgdXxxCj"},"source":["# Build a criterium with the str.contains method\n","crit = df.Vocation.str.contains('manager', na=False)\n","\n","# Create the new dataframe object and print the first 5 rows to the screen\n","vocation_df = df.loc[crit]\n","vocation_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tg-AWOi_zd4V"},"source":["We note that the term *manager* appear in all the values for the *Vocation* column."]},{"cell_type":"markdown","metadata":{"id":"6jxgYDE1z3Xh"},"source":["## UPDATING OR CHANGING THE VALUES IN A DATAFRAME"]},{"cell_type":"markdown","metadata":{"id":"yraY4tU95NBY"},"source":["Another valueble skill is to be able to change actual data in a dataframe object.  Fortunately, datadrame objects can be manipulated in many ways.  We begin by looking at changes to the column names."]},{"cell_type":"markdown","metadata":{"id":"Fu62F5-r7cQn"},"source":["### RENAMING COLUMNS"]},{"cell_type":"markdown","metadata":{"id":"2mvbZz5E7e9g"},"source":["We can replace the names of individual columns with the `rename` method using a dictionary.  Below we change *Name* to *Participant*.  For changes to be permanent we need to change the default `inplace` argument value to `True`."]},{"cell_type":"code","metadata":{"id":"9liOETe36ZfE"},"source":["df.rename(columns={'Name':'Participant'}, inplace=True)\n","df.columns"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pSIfSNwW778K"},"source":["df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SiBGCxJvDBBe"},"source":["### ADD $2$ TO EACH AGE VALUE"]},{"cell_type":"markdown","metadata":{"id":"T1bkSaOgDEp-"},"source":["In specific types of research, personal data are obfuscated to protect the privacy of the people in the dataset.  In a simple case, we might decide to subtract 2 from the age of every patient.  In reality, they are all 2 years older.  To fix this prior to data analysis, we must add 2 to each age."]},{"cell_type":"markdown","metadata":{"id":"iySLQpwyDcEX"},"source":["There are more than one way to achieve our goal. One way is to create a function and then use the `apply` method to apply the function to the values in a column."]},{"cell_type":"markdown","metadata":{"id":"XqHHc_ptCKCQ"},"source":["User-defined functions are created using thd `def` and `return` keywords. The former tells Python that we are about the create a new function. After a space follows the name we want to givw to our function. A set of parentheses follow that contains a placeholder for the argument. In its simplest form, the latter keyword follows. As the name indicates thsi section returns a value for our function. Below, it is clear from the code that `x` will hold an argument value. The function then adds $2$ to the argument value."]},{"cell_type":"code","metadata":{"id":"BldqVzdaCdMK"},"source":["def add2(x):\n","  return x + 2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HSmHXbcWDDSi"},"source":["The first five age values are printed below using the `head` method for the `df.Age` series."]},{"cell_type":"code","metadata":{"id":"dk2aFrliqsAn"},"source":["df.Age.head()  # Before"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PCMtGfVSDOlw"},"source":["The `apply` method is now used and the `add2` function is used. The value in each row is now increased by $2$."]},{"cell_type":"code","metadata":{"id":"quhn_UjLDq9u"},"source":["df.Age = df.Age.apply(add2)\n","df.Age.head()  # After"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D4udv1Y3EQql"},"source":["The `lambda` function in Python is a quick albeit more advanced way to achieve our goal. It create a nameless function.  Below, we subtract 2 from every *Age* entry to get back to where we started."]},{"cell_type":"code","metadata":{"id":"HXrPGenAEA1r"},"source":["df.Age = df.Age.apply(lambda x: x - 2)\n","df.Age.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"usV8Q0IlDxn-"},"source":["The simplest way to add a value would be to simply refer to a column (a series object) and overwrite it. Remember that the `=` assignment operator assigns what is to its right to what is to its left. Below, we use series notation to overwrite the `Age` column by adding $2$ to each row."]},{"cell_type":"code","metadata":{"id":"iDk2FuTSDuuZ"},"source":["df.Age = df.Age + 2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SrMOCcXzHIWn"},"source":["### CHANGE NOMINAL VARIABLE TO ORDINAL VARIABLE"]},{"cell_type":"markdown","metadata":{"id":"gi0QX5W-HYua"},"source":["For the purposes of encoding, we might want to change all *Active* values to 0 and *Control* values to 1 in the *Group* column.  To do this, we could use the `map` method and then pass a dictionary object as argument. The dictionary holds key value pairs. The key is the old value and the value is the new value."]},{"cell_type":"code","metadata":{"id":"5OwmFnEaEN0I"},"source":["df.Group = df.Group.map({'Control':0, 'Active':1})\n","df.Group.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wDh9YXiAH_jH"},"source":["One *problem* with the `map` method is that it will delete the entries (rows) for values that we do not specify.  To keep the original data when not specified, we can use the `replace` method instead."]},{"cell_type":"markdown","metadata":{"id":"3elfJ6XHKv3R"},"source":["## CHANGING COLUMNS"]},{"cell_type":"markdown","metadata":{"id":"8fBzilFwK4IO"},"source":["Adding columns is an often used technique when changing column. It is as simple as stating the new name in square brackets as a string and then adding a list of values.  We need it to be the same length (number of rows) as the dataframe."]},{"cell_type":"markdown","metadata":{"id":"VCEDKauILEoB"},"source":["### SPLITTING THE `PATIENT` COLUMN INTO A `FirstName` and `LastName` COLUMN"]},{"cell_type":"markdown","metadata":{"id":"V5K7xZ-eMCzP"},"source":["Below, we create two new columns called *FirstName* and *LastName* from the *Participant* column, splitting on the space using the `str.split` method."]},{"cell_type":"code","metadata":{"id":"WwKwQPZVrghO"},"source":["new_data = df.Participant.str.split(' ', expand=True)\n","df['FirstName'] = new_data[0]\n","df['LastName'] = new_data[1]\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9lye1BtqtPqW"},"source":["We can also combine two columns into one. Below, we use string concatination, combining the last name, a comma with a space (as a string) and the first name."]},{"cell_type":"code","metadata":{"id":"g_vchCYbIj3T"},"source":["df['Name'] = df.LastName + ', ' + df.FirstName\n","df.Name.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7avpdWf1MMui"},"source":["### CREATE A CATEGORICAL VARIABLE FROM NUMERICAL DATA"]},{"cell_type":"markdown","metadata":{"id":"ubBcGfQOMSFT"},"source":["Below, we create three sample space elements: *low*, *intermediate*, and *high* for the *CholesterolBefore* value of each patient.  To do so, we use the pandas `cut` function with specified bins. To understand the concept of bins, we start by looking at the minimum and maximum values."]},{"cell_type":"code","metadata":{"id":"DKbun2pmLRFi"},"source":["df.CholesterolBefore.min()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g-IvFZsmMhCZ"},"source":["df.CholesterolBefore.max()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0wxxwDLENZRw"},"source":["With the `bins=3` argument, we create three equally sized bins in the range form $1.2$ to $11.1$."]},{"cell_type":"markdown","metadata":{"id":"vRMl7NEpF3Ar"},"source":["In the code below, we create a new variable called `CholesterolBeforeLevel`. We use the pandas `cut` function with three arguments. The first is a pandas series object (the colum of interest). The second is the number of bins and the last is a list of names for each of the three bins."]},{"cell_type":"code","metadata":{"id":"qmyNnBRiMi8K"},"source":["# Non-physiological binning of cholesterol values\n","df['CholesterolBeforeLevel'] = pd.cut(df.CholesterolBefore, bins=3, labels=['low', 'intermediate', 'high'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6F7fFRVpNXz-"},"source":["Below, we view the first 10 new categorical values and actual values as a numpy array object."]},{"cell_type":"code","metadata":{"id":"_UDoxymWNHjp"},"source":["df[['CholesterolBefore', 'CholesterolBeforeLevel']].head(10).to_numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hLglHq4WHm3g"},"source":["These three bins are non-physiological in that we have specific values for low, normal, and high levels of cholesterol. To control the bin values, we can specify the bin cut-off values as a list. To understand this we need to know about open and closed intervals. An open interval such as $\\left( 10 , 20 \\right)$ means that neither $10$ nor $20$ are included. Inclusion requires a closed interval, denoted by $\\left[ 10 , 20 \\right]$. We also have half-open intervals. In $\\left( 10 , 20 \\right]$, $10$ is not included but $20$ is. We can also have $\\left( 10, 20 \\right]$, with $10$ not included and $20$ being included."]},{"cell_type":"markdown","metadata":{"id":"FJG351AqIjaG"},"source":["Consider then the values $11.2, 12.2, 13.2, 15, 16, 16, 19.2, 20$. Imagine then that everything below $13$ is _low_, $13$ to below $16$ is normal and $16$ and above is high. Proper intervals would then be $\\left[ 11.2 , 13 \\right)$. Here we use the lowest value as the inlcuded lower bound, but $13$ is not included. A value of $13$ would be in the second bin. The second bin would have bounds $\\left[ 13 , 16 \\right)$ and the last $\\left[ 16 , 20 \\right]$."]},{"cell_type":"markdown","metadata":{"id":"nNyng7I7Jems"},"source":["In pandas we can only have left or right half-open intervals. The keyword argument `right` is set to `False` by default, with right open intervals."]},{"cell_type":"markdown","metadata":{"id":"E7LEU1qHNnOf"},"source":["There is a `right=True` argument value.  It states that the intervals are right-closed so that `bins=[10,20,30]` would mean intervals (10,20] $10$ not being included but $20$ being included here and (20,30] $20$.  Similarly the `include_lowest=False` argument means that the left-most value is not included (the 10 in this explanation).  Set the argument to `True` to have the first interval be [10,20]."]},{"cell_type":"markdown","metadata":{"id":"3BEVeMcKvGk2"},"source":["Below, we create three bins with intervals low = $[0, 2.5)$, normal = $[2.5, 5.0)$, and high = $[5.0,20)$.  So, if a patient has a cholesterol value of 5, they would fall in the high group. Note that $20$ is above the maximum value and is a safe value to use. Note also that there are four numbers for three bins."]},{"cell_type":"code","metadata":{"id":"g0Zg266_uiD-"},"source":["df.CholesterolBeforeLevel = pd.cut(df.CholesterolBefore,\n","                                   bins=[0,5,10,20],\n","                                   right=False,\n","                                   labels=['low', 'normal', 'high'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FK30SVW8OvXe"},"source":["### DELETE A COLUMN"]},{"cell_type":"markdown","metadata":{"id":"GgafO9yXOxSP"},"source":["Deleting a column can be achieved using the `drop` method.  To make the deletion permamant, we use the `inplace=True` argument.  Let's delete our newly created *Name* column."]},{"cell_type":"code","metadata":{"id":"lCnQMPQBNOGC"},"source":["df.drop(columns=['Name'], inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lJrzecpGO9gw"},"source":["df.columns"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4XMAnQ31ByO3"},"source":["## SORTING"]},{"cell_type":"markdown","metadata":{"id":"hmlND79aB0kp"},"source":["Sorting can be a useful way to interact with our data.  Below, we change the dataframe object by sorting the *LastNames* alphabetically.  All the corresponing column will change as well, so that each row still pertains to the same patient. "]},{"cell_type":"code","metadata":{"id":"OD3FWLT7PPGj"},"source":["df.sort_values(by='LastName')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rEpPPW1iDQPT"},"source":["The alphabetical order can be reversed by using the `acending=False` argument."]},{"cell_type":"code","metadata":{"id":"XSP9ykWlCQ-9"},"source":["df.sort_values(by='LastName', ascending=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fWKpiHwID7oY"},"source":["We can sort by more than one column at a time.  This is done by passing a list of column names.  Below, we sort by *Age* and the *sBP*.  With default values, numerical and date values will be from smaller to larger values and from earlier to later dates and categorical variables will be alphabetical."]},{"cell_type":"code","metadata":{"id":"BIVFQtSDDkf6"},"source":["df.sort_values(by=['Age', 'sBP'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R8GiBubBETaq"},"source":["The three participants aged 30 now have their systolic blood pressure values in ascending order."]},{"cell_type":"markdown","metadata":{"id":"Gl6ho_zdE8Mq"},"source":["Not all the column names passed as a list to sort by, need be in the same order.  We can also pass a list with corresponding order."]},{"cell_type":"code","metadata":{"id":"-M-bWB4dEKZH"},"source":["# Sort Age in ascending and sBP in descending order\n","df.sort_values(by=['Age', 'sBP'], ascending=[True, False])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s-nbT6bdFfBH"},"source":["The three patients aged 30 are now sorted by the highest systolic blood pressure first."]},{"cell_type":"markdown","metadata":{"id":"nG-yjSeoFksG"},"source":["The `sort_value` method does not make permanent changes to the dataframe, unless the argument `inplace` (which is set to `False` by default) is set to `True`."]},{"cell_type":"markdown","metadata":{"id":"jiHGAR3FHgPO"},"source":["The `nlargest` method is useful if we only want to view the highest numerical values in a column.  Below, we look at the 15 highest systolic blood pressure values."]},{"cell_type":"code","metadata":{"id":"s-1kxAh4GK7i"},"source":["df.sBP.nlargest(15)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3X2zqnhOIhOh"},"source":["We can reverse the order of the syntax above a bit, if we want to see the rest of the columns too."]},{"cell_type":"code","metadata":{"id":"WdR26QK7H8qp"},"source":["# Column is listed as arguemnt\n","df.nlargest(10, 'sBP')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xVEi4VaIYKot"},"source":["If we want the smallest values, there is also a `nsmallest` method."]},{"cell_type":"markdown","metadata":{"id":"CoWSXiMSY46v"},"source":["## MISSING VALUES"]},{"cell_type":"markdown","metadata":{"id":"78HG7riOsIdI"},"source":["### THE NUMPY `nan` VALUE"]},{"cell_type":"markdown","metadata":{"id":"A2pHBRAXY7zp"},"source":["It is very often that datasets contain missing data.  The numpy library has a specific entity called a `nan` value.  This stands for *not a number*.  Below, we see it by itself and also as an element in a Python list."]},{"cell_type":"code","metadata":{"id":"FnfqML1_ZPs1"},"source":["np.nan"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wb4Dw18uZUVx"},"source":["my_list = [1, 2, 3, np.nan]\n","my_list"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5FIbO-aMZg7W"},"source":["The list object, `my_list`, above, cannot be used as argument to functions such as `sum`, since Python does not know how to deal with this missing data.  Below, we use the numpy `sum` function.  The results is a `nan` value."]},{"cell_type":"code","metadata":{"id":"Gco8fJbcZyug"},"source":["np.sum(my_list)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HmpdVZKuaV_-"},"source":["Now, let's have a look at how pandas deals with misssing values.  We will import another spreadsheet file that contains missing data."]},{"cell_type":"markdown","metadata":{"id":"SL355xp3sNi7"},"source":["### A DATAFRAME WITH MISSING DATA"]},{"cell_type":"code","metadata":{"id":"wPdXGq7ial50"},"source":["missing_df = pd.read_csv('MissingData.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"37di8MeTpVbT"},"source":["The DataFrame has the following columns: `age`, `salary`, and `previous_company`  Most of the columns are self-explanatory.  The *previous_company* indicates whether the person had previously used a different investment company instead of ours or had no investment at all."]},{"cell_type":"markdown","metadata":{"id":"A5pbS2e7hmnG"},"source":["When we print the dataframe object, we note all the `NaN` values, which pandas uses to indicate missing data."]},{"cell_type":"code","metadata":{"id":"S8Jx_tpzdS7J"},"source":["missing_df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JKAMXRCFsS_2"},"source":["### DELETING MISSING DATA"]},{"cell_type":"markdown","metadata":{"id":"QFzsNxVRnHg6"},"source":["The first way of dealing with missing data, is to simply remove all the rows that contain any missing data.  This is done with the `.dropna()` method.  To make the changes permanent, we would have to use the `inplace=True` argument. Instead of permanent removal, we create a new dataframe object."]},{"cell_type":"code","metadata":{"id":"CGgznJrDnZtC"},"source":["complete_data_df = missing_df.dropna() # Non permanent removal\n","complete_data_df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r4G2geIXnkBO"},"source":["There is another argument for this method, `how` that is set to `any`.  This default states that if any of the values in a row are missing, the whole row is dropped.  There is also an `all` value for this argument that will only remove a row if all the values are missing."]},{"cell_type":"markdown","metadata":{"id":"5FJ99CWzoGe5"},"source":["Another argument is `axis`.  By default this is set to `0` or `index`, which indicates that we are interested in dropping rows.  When set to `1` or `columns`, columns will be dropped."]},{"cell_type":"markdown","metadata":{"id":"aIBqxFOCozrG"},"source":["We can constrain which columns to include when checking for missing values, using the `subset` argument."]},{"cell_type":"code","metadata":{"id":"RTxBraJQo_2q"},"source":["missing_df.dropna(subset=['age'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y1eVKCu9pG5E"},"source":["We see that there are still missing data in the *salary* and *previous_company* collumns."]},{"cell_type":"markdown","metadata":{"id":"uvWUgTGKrYke"},"source":["To find out how many rows contain missing data, we can make use of the fact that `True` and `False` are represented by 1 and 0 and can thus be added.  The `isna` method will return Boolen values depending on whether the data is missing."]},{"cell_type":"code","metadata":{"id":"738C9Jq9rnYL"},"source":["missing_df.age.isna()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mus3I0iQry12"},"source":["We can sum over these Boolean values using the `sum` method. Since `True` values are saved internally to Python as the value $1$, the sum will be the number of values marked as `True` when missing, which as we saw, is what the `isna` method returns."]},{"cell_type":"code","metadata":{"id":"j42b3Ikcr1qM"},"source":["missing_df.age.isna().sum()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"azgatyURr8ro"},"source":["We see that there are $4$ missing values in the *age* column."]},{"cell_type":"markdown","metadata":{"id":"xVxphXr4sXc-"},"source":["### REPLACING MISSING VALUES"]},{"cell_type":"markdown","metadata":{"id":"YVngdKTNsaJU"},"source":["The process of creating values to fill in missing data is called **data imputation** and is a seperate and complicated subject.  The pandas library provides a `fillna` method for filling in the missing data with simple calculations."]},{"cell_type":"markdown","metadata":{"id":"GKCG7Svvs-lr"},"source":["Below we use the argument and value `method=ffill` which simply fill empty values with previous value.  There is also a `method=bfill` argument setting that fills the missing data with the next available data down the column."]},{"cell_type":"code","metadata":{"id":"mf9EHpSJsrlT"},"source":["missing_df.age.fillna(method='ffill')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RcdI-H6RtSVv"},"source":["We can also specify a specific value.  For numerical data this could be the median for that variable and for categorical data, it might be the mode.  We will learn about summary statistics in the next notebook.  For now, we will use the `median` method.  It calculate the median for a column with numerical data, ignoring missing data automatically."]},{"cell_type":"code","metadata":{"id":"1pEZACJMuCHa"},"source":["# The median age (pandas ignores the missing values)\n","missing_df.age.median()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DMf3HT4_uIri"},"source":["We can now impute the missing ages with this median."]},{"cell_type":"code","metadata":{"id":"bkKa-rAHuFKx"},"source":["missing_df.age.fillna(missing_df.age.median())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EwkgMUuuuPfE"},"source":["If we want the changes to be permanent, we have to use the `inplace=True` argument."]},{"cell_type":"markdown","metadata":{"id":"46y0vtPHvj2k"},"source":["### DEFAULT MISSING DATA"]},{"cell_type":"markdown","metadata":{"id":"X42Lmdkyvlzq"},"source":["It is common to use default values when data is not available at the time of capture.  If we know what these are, we can interpret them as missing data when the spreadsheet file is imported."]},{"cell_type":"markdown","metadata":{"id":"RY0hSoajvvYf"},"source":["Below, we import a spreadsheet file that uses `999`, `Nil`, and `Missing` for missing values instead of leaving the spreadsheet cell blank."]},{"cell_type":"code","metadata":{"id":"vQAF8Cb6v8j6"},"source":["default_missing_df = pd.read_csv('DefaultMissingData.csv')\n","default_missing_df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cicFYJPfgUz8"},"source":["We can replace the missing values or specify all the words and numbers used for coding missing data when we import the data file."]},{"cell_type":"code","metadata":{"id":"lQuYym23gfMi"},"source":["default_missing_df = pd.read_csv('DefaultMissingData.csv', na_values=[999, 'Nil', 'Missing'])\n","default_missing_df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YoM47mVEgm80"},"source":["Those values ar now `NaN`."]},{"cell_type":"markdown","metadata":{"id":"QLGol1A7RDqj"},"source":["## WORKING WITH DATES AND TIMES"]},{"cell_type":"markdown","metadata":{"id":"109D4xl1RHCq"},"source":["In this section, we import a new spreadsheet file.  It contains data on dates and times of biological laboratory investigations."]},{"cell_type":"code","metadata":{"id":"hZj1uayGRMoO"},"source":["dt = pd.read_csv('DatesTimes.csv')\n","dt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zA7S73wDRbRA"},"source":["Let's take a look at the data types."]},{"cell_type":"code","metadata":{"id":"sOVUajlcRX7O"},"source":["dt.dtypes"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-hnz4-zoRecG"},"source":["The *SpecimenDate*, *TestDate* and the *TestTime* columns contain objects instead of datetime objects.  We can convert these into a proper datetime data type.  We will do so by creating a new variable (column header) that combines the two of the columns."]},{"cell_type":"code","metadata":{"id":"zI6L0R02SugD"},"source":["dt['DateTime'] = dt.TestDate + ' ' + dt.TestTime  # Add a space\n","dt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eP4InzpgTHNL"},"source":["This new variable is still an object."]},{"cell_type":"code","metadata":{"id":"LYx2cG6ITKhl"},"source":["dt.DateTime.dtype"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"alszcrxxXSW5"},"source":["We will now create a new column and use the pandas `to_datetime()` function to convert the object (copied from the *DateTime* column).  The `format=` argument allows us to specify the exact format that the object was in. The format of the data in the `DateTime` column is YYYY/MM/DD HH:MM. We use pandas code to indicate these. Uppercase `%Y` specifies the full year, i.e. `2025` instead of just `25`. The rest of the symbols are self explanatory."]},{"cell_type":"code","metadata":{"id":"VEAFSAK5RvFS"},"source":["dt['datetime'] = pd.to_datetime(dt.DateTime, format='%Y/%m/%d %H:%M')\n","dt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LVmNJ0CEiafb"},"source":["The new `datetime` column is now a datetime object."]},{"cell_type":"code","metadata":{"id":"778NC73PiX7d"},"source":["dt.dtypes"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L1QGFNjPT7U6"},"source":["Now that this is a datetime object, we might want to analyze this data by month of test.  To do so, we create a new column containing the month and use the `dt.month_name` method. We also shorten the month to the first three letters using the `str.slice` method with the `stop` argument set to `3`."]},{"cell_type":"code","metadata":{"id":"9YAL9ROPUGoW"},"source":["dt['month'] = dt.datetime.dt.month_name().str.slice(stop=3)\n","dt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rW7LgWjJX1xO"},"source":["There are various other values, we can extract from the datetime object."]},{"cell_type":"code","metadata":{"id":"eQoCleGWX2UD"},"source":["dt.datetime.dt.year  # The year"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1N7xC_Z8YGO3"},"source":["dt.datetime.dt.hour  # The hour"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mleCRiDfYRUD"},"source":["## CONCLUSION"]},{"cell_type":"markdown","metadata":{"id":"fIaUQx4RYSol"},"source":["Pandas is a powerful library and we are going to use all the techniques that we have learned about here to manipulate our data in order to do analyses on it."]},{"cell_type":"markdown","metadata":{"id":"7HOHCktea9_8"},"source":["There is still a lot we have to learn over and above this, though.  We will do so, whilest analysing our data."]},{"cell_type":"code","metadata":{"id":"Uu7laauuIsSI"},"source":[""],"execution_count":null,"outputs":[]}]}